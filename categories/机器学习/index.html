<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: 机器学习 - blog</title><link rel="manifest" href="../../manifest.json"><meta name="application-name" content="blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="blog"><meta property="og:url" content="https://0914ds.github.io/"><meta property="og:site_name" content="blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://0914ds.github.io/img/og_image.png"><meta property="article:author" content="albert dong"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://0914ds.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://0914ds.github.io"},"headline":"blog","image":["https://0914ds.github.io/img/og_image.png"],"author":{"@type":"Person","name":"albert dong"},"publisher":{"@type":"Organization","name":"blog","logo":{"@type":"ImageObject","url":"https://0914ds.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="../../img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="../../css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="atom.xml" title="blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="../../index.html"><img src="../../img/logo.svg" alt="blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="../../index.html">Home</a><a class="navbar-item" href="../../archives">Archives</a><a class="navbar-item" href="../../categories">Categories</a><a class="navbar-item" href="../../tags">Tags</a><a class="navbar-item" href="../../about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="../../categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">机器学习</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-17T17:13:16.000Z" title="2023/8/18 01:13:16">2023-08-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-17T17:30:11.349Z" title="2023/8/18 01:30:11">2023-08-18</time></span><span class="level-item"><a class="link-muted" href="">机器学习</a></span><span class="level-item">an hour read (About 7909 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="../../2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">多元线性回归</a></p><div class="content"><meta name="referrer" content="no-referrer">
<h2 id="梯度下降">梯度下降</h2>
<h3 id="线性回归预测房价">线性回归预测房价</h3>
<ul>
<li>数据加载</li>
<li>数据介绍</li>
<li>数据拆分</li>
<li>数据建模</li>
<li>数据预测</li>
<li>数据评估</li>
</ul>
<h3 id="无约束最优化问题">1、无约束最优化问题</h3>
<h4 id="无约束最优化">1.1、无约束最优化</h4>
<p> <strong>  无约束最优化问题</strong>（unconstrained optimization
problem）指的是从一个问题的所有<strong>可能</strong>的备选方案中，选择出依某种指标来说是<strong>最优</strong>的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函<span class="math inline">\(J(\theta)\)</span>的极小化或极大化问题：<strong>广义上</strong>，最优化包括数学规划、图和网络、组合最优化、库存论、决策论、排队论、最优控制等。<strong>狭义上</strong>，最优化仅指数学规划。</p>
<h4 id="梯度下降-1">1.2、梯度下降</h4>
<p>  <strong>梯度下降法</strong>(Gradient
Descent)是一个算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常<strong>通用</strong>的优化算法来帮助一些机器学习算法（都是无约束最优化问题）求解出<strong>最优解</strong>，
所谓的通用就是很多机器学习算法都是用梯度下降，甚至<strong>深度学习</strong>也是用它来求解最优解。所有优化算法的目的都是期望以<strong>最快</strong>的速度把模型参数θ求解出来，梯度下降法就是一种<strong>经典</strong>常用的优化算法。</p>
<p>  之前利用正规方程求解的 θ 是最优解的原因是 MSE
这个损失函数是凸函数。但是，机器学习的损失函数并非都是凸函数，设置导数为
0 会得到很多个极值，不能确定唯一解。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ac185dc9b60d4411b6d0c9ae9dc15543.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  使用正规方程 <span class="math inline">\(\theta =
(X^TX)^{-1}X^Ty\)</span> 求解的另一个限制是特征维度（<span class="math inline">\(X_1、X_2……、X_n\)</span>）不能太多，矩阵逆运算的时间复杂度通常为
<span class="math inline">\(O(n^3)\)</span>
。换句话说，就是如果特征数量翻$
2^3$倍，你的计算时间大致为原来的倍，也就是之前时间的8倍。举个例子，2
个特征 1 秒，4 个特征就是 8 秒，8 个特征就是 64 秒，16 个特征就是 512
秒，当特征更多的时候呢？运行时间会非常漫长~</p>
<p>  所以正规方程求出最优解<strong>并不是</strong>机器学习甚至深度学习常用的手段。</p>
<p>  之前我们令导数为 0，反过来求解最低点 θ
是多少，而梯度下降法是<strong>一点点</strong>去逼近最优解!</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/6e97ba09e2694bf69d2919cd159a6a59.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  其实这就跟生活中的情形很像，比如你问一个朋友的工资是多少，他说你猜？那就很难了，他说你猜完我告诉你是猜高了还是猜低了，这样你就可以奔着对的方向一直猜下去，最后总会猜对！梯度下降法就是这样的，多次尝试。并且，在试的过程中还得想办法知道是不是在猜对的路上，说白了就是得到正确的反馈再调整然后继续猜才有意义~</p>
<p>  这个就好比道士下山，我们把 Loss
（或者称为Cost，即损失）曲线看成是<strong>山谷</strong>，如果走过了，就再
往回返，所以是一个迭代的过程。</p>
<h4 id="梯度下降公式">1.3、梯度下降公式</h4>
<p>  这里梯度下降法的公式就是一个式子指导计算机迭代过程中如何去调整<span class="math inline">\(\theta\)</span>，可以通过泰勒公式一阶展开来进行推导和证明：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/680d3e456f084caa87d35c55ece68b98.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  这里的 <span class="math inline">\(w_j\)</span> 就是 <span class="math inline">\(\theta\)</span> 中的某一个 j = 0...m，这里的 <span class="math inline">\(\eta\)</span> 就是梯度下降图里的 learning
step，很多时候也叫学习率 learning rate，很多时候也用 <span class="math inline">\(\alpha\)</span>
表示，这个学习率我们可以看作是下山迈的<strong>步子</strong>的大小，步子迈的大下山就快。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/22d31ba2d7c348bca74593141837f569.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  学习率一般都是<strong>正数</strong>，如果在山左侧（曲线<strong>左半边</strong>）梯度是负的，那么这个负号就会把
<span class="math inline">\(w_j\)</span> 往大了调，
如果在山右侧（曲线右半边）梯度就是正的，那么负号就会把 <span class="math inline">\(w_j\)</span> 往小了调。每次 <span class="math inline">\(w_j\)</span> 调整的幅度就是 <span class="math inline">\(\eta *
gradient\)</span>，就是横轴上移动的距离。</p>
<p>  因此，无论在左边，还是在右边，梯度下降都可以快速找到最优解，实现快速<strong>下山</strong>~</p>
<p>  如果特征或维度越多，那么这个公式用的次数就越多，也就是每次迭代要应用的这个式子多次（多少特征，就应用多少次），所以其实上面的图不是特别准，因为
<span class="math inline">\(\theta\)</span>
对应的是很多维度，应该每一个维度都可以画一个这样的图，或者是一个多维空间的图。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/66ce3adba54149a382512476bf1da237.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9f2ec32dd16d40ceb75d032cbe6d3ea9.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  所以观察上图我们可以发现不是某一个 <span class="math inline">\(\theta_0\)</span> 或 <span class="math inline">\(\theta_1\)</span>
找到最小值就是最优解，而是它们一起找到 <span class="math inline">\(J(\theta)\)</span> 最小值才是最优解。</p>
<h4 id="学习率">1.4、学习率</h4>
<p>  根据我们上面讲的梯度下降公式，我们知道 <span class="math inline">\(\eta\)</span> 是学习率，设置大的学习率 <span class="math inline">\(w_j\)</span> 每次调整的幅度就大，设置小的学习率
<span class="math inline">\(w_j\)</span>
每次调整的幅度就小，然而如果步子迈的太大也会有问题，俗话说步子大了容易扯着蛋！学习率大，可能一下子迈过了，到另一边去了（从曲线左半边跳到右半边），继续梯度下降又迈回来，
使得来来回回震荡。步子太小呢，就像蜗牛一步步往前挪，也会使得整体迭代次数增加。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83476c288f4d4e84951997fc776a8fa1.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  学习率的设置是门一门学问，一般我们会把它设置成一个比较小的正整数，0.1、0.01、0.001、0.0001，都是常见的设定数值（然后根据情况调整）。一般情况下学习率在整体迭代过程中是不变，但是也可以设置成随着迭代次数增多学习率逐渐变小，因为越靠近山谷我们就可以步子迈小点，可以更精准的走入最低点，同时防止走过。还有一些深度学习的优化算法会自己控制调整学习率这个值，后面学习过程中这些策略在讲解代码中我们会一一讲到。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c782a1c81fb240c89ef39bf55d74d3e1.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="全局最优化">1.5、全局最优化</h4>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/7d59daf9da474ef9b1716191f47064c8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>上图显示了梯度下降的两个主要挑战：</p>
<ul>
<li>若随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值；</li>
<li>若随机初始化，算法从右侧起步，那么需要经过很长时间才能越过Plateau（函数停滞带，梯度很小），如果停下得太早，则永远达不到全局最小值；</li>
</ul>
<p>  而线性回归的模型MSE损失函数恰好是个凸函数，凸函数保证了只有一个全局最小值，其次是个连续函数，斜率不会发生陡峭的变化，因此即便是乱走，梯度下降都可以趋近全局最小值。</p>
<p>  上图损失函数是非凸函数，梯度下降法是有可能落到局部最小值的，所以其实步长不能设置的太小太稳健，那样就很容易落入局部最优解，虽说局部最小值也没大问题，
因为模型只要是<strong>堪用</strong>的就好嘛，但是我们肯定还是尽量要奔着全局最优解去！</p>
<h4 id="梯度下降步骤">1.6、梯度下降步骤</h4>
<p>梯度下降流程就是“猜”正确答案的过程:</p>
<ul>
<li><p>1、“瞎蒙”，Random 随机数生成 <span class="math inline">\(\theta\)</span>，随机生成一组数值 <span class="math inline">\(w_0、w_1……w_n\)</span> ，期望 <span class="math inline">\(\mu\)</span> 为 0 方差 <span class="math inline">\(\sigma\)</span> 为 1 的正太分布数据。</p></li>
<li><p>2、求梯度 g
，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p></li>
<li><p>3、if g &lt; 0, <span class="math inline">\(\theta\)</span>
变大，if g &gt; 0, <span class="math inline">\(\theta\)</span>
变小</p></li>
<li><p>4、判断是否收敛，如果收敛跳出迭代，如果没有达到收敛，回第 2
步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p></li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/51536cae3e524a419ab6feff64e5df31.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="代码模拟梯度下降">1.7、代码模拟梯度下降</h4>
<ul>
<li><p>梯度下降优化算法，比正规方程，应用更加广泛</p></li>
<li><p>什么是梯度？</p>
<ul>
<li>梯度就是导数对应的值！</li>
</ul></li>
<li><p>下降？</p>
<ul>
<li>涉及到优化问题，最小二乘法</li>
</ul></li>
<li><p>梯度下降呢？</p>
<ul>
<li>梯度方向下降，速度最快的~</li>
</ul></li>
</ul>
<p>  接下来，我们使用代码来描述上面梯度下降的过程：</p>
<p>方程如下：</p>
<p><span class="math inline">\(f(x) = (x - 3.5)^2 - 4.5x +
10\)</span></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9d04508431a94e4f9a54bed4c6e01dc8.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>使用梯度下降的思想，来一步步逼近，函数的最小值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">f = <span class="keyword">lambda</span> x : (x - <span class="number">3.5</span>)**<span class="number">2</span> -<span class="number">4.5</span>*x + <span class="number">10</span></span><br><span class="line"><span class="comment"># 导函数</span></span><br><span class="line">d = <span class="keyword">lambda</span> x :<span class="number">2</span>*(x - <span class="number">3.5</span>) - <span class="number">4.5</span> <span class="comment"># 梯度 == 导数</span></span><br><span class="line"><span class="comment"># 梯度下降的步幅，比例，（学习率，幅度）</span></span><br><span class="line">step = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 求解当x等于多少的时候，函数值最小。求解目标值：随机生成的</span></span><br><span class="line"><span class="comment"># 相等于：&#x27;瞎蒙&#x27; ----&gt; 方法 ----&gt; 优化</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">12</span>,size = <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 梯度下降，每下降一步，每走一步，目标值，都会更新。</span></span><br><span class="line"><span class="comment"># 更新的这个新值和上一步的值，差异，如果差异很小（万分之一）</span></span><br><span class="line"><span class="comment"># 梯度下降退出</span></span><br><span class="line">last_x = x + <span class="number">0.02</span> <span class="comment"># 记录上一步的值，首先让last_x和x有一定的差异！！！</span></span><br><span class="line"><span class="comment"># 精确率，真实计算，都是有误差，自己定义</span></span><br><span class="line">precision = <span class="number">1e-4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;+++++++++++++++++++++&#x27;</span>, x)</span><br><span class="line">x_ = [x]</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 退出条件，精确度，满足了</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(x - last_x) &lt; precision:</span><br><span class="line">        <span class="keyword">break</span>   </span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    last_x = x</span><br><span class="line">    x -= step*d(x) <span class="comment"># 更新，减法：最小值</span></span><br><span class="line">    x_.append(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------&#x27;</span>,x)</span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = <span class="string">&#x27;Kaiti SC&#x27;</span></span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">x = np.linspace(<span class="number">5.75</span> - <span class="number">5</span>, <span class="number">5.75</span> + <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">plt.plot(x,y,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;梯度下降&#x27;</span>,size = <span class="number">24</span>,pad = <span class="number">15</span>)</span><br><span class="line">x_ = np.array(x_)</span><br><span class="line">y_ = f(x_)</span><br><span class="line">plt.scatter(x_, y_,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;./图片/5-梯度下降.jpg&#x27;</span>,dpi = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<p>函数的最优解是：<strong>5.75</strong>。你可以发现，随机赋值的变量 x
，无论<strong>大于</strong>5.75，还是<strong>小于</strong>5.75，经过梯度下降，最终都慢慢靠近5.75这个最优解！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c3a7bf82304d4cd990306bccc9e33e98.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/29a4d4047fa24ea495f55630a6c6d174.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>注意：</strong></p>
<ol type="1">
<li>梯度下降存在一定误差，不是完美解~</li>
<li>在误差允许的范围内，梯度下降所求得的机器学习模型，是堪用的！</li>
<li>梯度下降的步幅step，不能太大，俗话说步子不能迈的太大！</li>
<li>精确度，可以根据实际情况调整</li>
<li>while True循环里面，持续进行梯度下降：</li>
</ol>
<p>   <span class="math inline">\(\theta = \theta - \eta
\frac{\partial}{\partial \theta}J(\theta)\)</span> 其中的 $$
叫做学习率</p>
<p>  <span class="math inline">\(x = x - \eta\frac{\partial}{\partial
x}f(x)\)</span></p>
<p>  <span class="math inline">\(x = x - step*\frac{\partial}{\partial
x} f(x)\)</span> 其中的 $step $ 叫做学习率</p>
<p>  <span class="math inline">\(x = x - step * f&#39;(x)\)</span></p>
<ol start="6" type="1">
<li>while
循环退出条件是：x更新之后和上一次相差绝对值小于特定精确度！</li>
</ol>
<h3 id="梯度下降方法">2、梯度下降方法</h3>
<h4 id="三种梯度下降不同">2.1、三种梯度下降不同</h4>
<p>梯度下降分三类：批量梯度下降BGD（<strong>Batch Gradient
Descent</strong>）、小批量梯度下降MBGD（<strong>Mini-Batch Gradient
Descent</strong>）、随机梯度下降SGD（<strong>Stochastic Gradient
Descent</strong>）。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83b78948c0074a17b4334e1ca691b8fc.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>三种梯度下降有什么不同呢？我们从梯度下降步骤开始讲起，梯度下降步骤分一下四步：</p>
<ul>
<li><p>1、随机赋值，Random 随机数生成 <span class="math inline">\(\theta\)</span>，随机一组数值 <span class="math inline">\(w_0、w_1……w_n\)</span></p></li>
<li><p>2、求梯度 g
，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p></li>
<li><p>3、if g &lt; 0, <span class="math inline">\(\theta\)</span>
变大，if g &gt; 0, <span class="math inline">\(\theta\)</span>
变小</p></li>
<li><p>4、判断是否收敛
convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2
步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p></li>
</ul>
<p>三种梯度下降不同，体现在第二步中：</p>
<ul>
<li>BGD是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新</li>
<li>MBGD是指在<strong>每次迭代</strong>使用<strong>一部分样本</strong>（所有样本500个，使用其中32个样本）来进行梯度的更新</li>
<li>SGD是指<strong>每次迭代</strong>随机选择<strong>一个样本</strong>来进行梯度更新</li>
</ul>
<h4 id="线性回归梯度更新公式">2.2、线性回归梯度更新公式</h4>
<p>回顾上一讲公式！</p>
<p>最小二乘法公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/49c3dfaf8c4b4a54ab80a7e51f90cfa6.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>矩阵写法：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/670e2e063398461e9143ae13b7c4709a.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>接着我们来讲解如何求解上面梯度下降的第 2
步，即我们要推导出损失函数的导函数来。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/038487f53e8f4c47af75de156a2e98c4.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  <span class="math inline">\(x^2\)</span>的导数就是
2x，根据链式求导法则，我们可以推出上面第（1）步。然后是多元线性回归，所以
<span class="math inline">\(h_{\theta}(x)\)</span> 就 是 <span class="math inline">\(\theta^Tx\)</span> 即是<span class="math inline">\(w_0x_0 + w_1x_1 + …… + w_nx_n\)</span> 即<span class="math inline">\(\sum\limits_{i =
0}^n\theta_ix_i\)</span>。到这里我们是对 <span class="math inline">\(\theta_j\)</span> 来求偏导，那么和 <span class="math inline">\(w_j\)</span> 没有关系的可以忽略不计，所以只剩下
<span class="math inline">\(x_j\)</span>。</p>
<p>  我们可以得到结论就是 <span class="math inline">\(\theta_j\)</span>
对应的梯度与预测值 <span class="math inline">\(\hat{y}\)</span> 和真实值
y 有关，这里 <span class="math inline">\(\hat{y}\)</span> 和 y
是列向量（即多个数据），同时还与 <span class="math inline">\(\theta_j\)</span> 对应的特征维度 <span class="math inline">\(x_j\)</span> 有关，这里 <span class="math inline">\(x_j\)</span> 是原始数据集矩阵的第 j
列。如果我们分别去对每个维度 <span class="math inline">\(\theta_0、\theta_1……\theta_n\)</span>
求偏导，即可得到所有维度对应的梯度值。</p>
<ul>
<li><span class="math inline">\(g_0 = (h_{\theta}(x) -
y)x_0\)</span></li>
<li><span class="math inline">\(g_1 = (h_{\theta}(x) -
y)x_1\)</span></li>
<li>……</li>
<li><span class="math inline">\(g_j = (h_{\theta}(x) -
y)x_j\)</span></li>
</ul>
<p><strong>总结：</strong></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/1e745bd465d542cc8593e12977a6cb26.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="批量梯度下降bgd">2.3、批量梯度下降BGD</h4>
<p>  <strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/bd716fec62c2485e831263b5d84f8197.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>去掉 <span class="math inline">\(\frac{1}{n}\)</span>
也可以，因为它是一个常量，可以和 <span class="math inline">\(\eta\)</span> 合并</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/52e7aec970ae4be887f2b6eb059e5a54.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>矩阵写法：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f983d092c8914a60876c80853bd08b43.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 𝑖 = 1, 2, ..., n 表示样本数， 𝑗 = 0,
1……表示特征数，<strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} = 1\)</span></strong>。</p>
<p><strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理！</strong></p>
<p><strong>优点：</strong>
  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
<strong>缺点：</strong>   （1）当样本数目 n
很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。</p>
<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/b23d78b73e8c4a88813c02917ff33853.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="随机梯度下降sgd">2.4、随机梯度下降SGD</h4>
<p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/4e0a2d7309a2482882c7c641014a98af.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>批量梯度下降</strong>算法每次都会使用<strong>全部</strong>训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而<strong>随机梯度下降</strong>算法每次只随机选择<strong>一个</strong>样本来更新模型参数，因此每次的学习是非常快速的。</p>
<p>  <strong>优点：</strong>
  （1）由于不是在全部训练数据上的更新计算，而是在每轮迭代中，随机选择一条数据进行更新计算，这样每一轮参数的更新速度大大加快。
  <strong>缺点：</strong>
  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。</p>
<p>  <strong>解释一下为什么SGD收敛速度比BGD要快：</strong>   *
这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）。
  *
而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被迭代30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。
  * 也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W
次。</p>
<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程就会盲目一些。其迭代的收敛曲线示意图可以表示如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/25e11b29bcdc48df880924c7e75c1b16.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="小批量梯度下降mbgd">2.5、小批量梯度下降MBGD</h4>
<p><strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个<strong>折中</strong>办法。其思想是：<strong>每次迭代</strong>使用总样本中的一部分（batch_size）样本来对参数进行更新。这里我们假设
batch_size = 32，样本数 n = 1000
。实现了更新速度与更新次数之间的平衡。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/608f1025518842188c8c47350e5e8ce8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>相对于随机梯度下降算法，小批量梯度下降算法降低了收敛波动性，
即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。</p>
<p>一般情况下，小批量梯度下降是梯度下降的推荐变体，特别是在深度学习中。每次随机选择2的幂数个样本来进行学习，例如：8、16、32、64、128、256。因为计算机的结构就是二进制的。但是也要根据具体问题而选择，实践中可以进行多次试验，
选择一个更新速度与更次次数都较适合的样本数。</p>
<p>MBGD梯度下降迭代的收敛曲线更加温柔一些：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/fa690fe760284383b198810802e82995.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="梯度下降优化">2.6、梯度下降优化</h4>
<p>虽然梯度下降算法效果很好，并且广泛使用，但是不管用上面三种哪一种，都存在一些挑战与问题，我们可以从以下几点进行优化:</p>
<ol type="1">
<li><p>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/0bbe5b04ddf24a78a71c4700cbde90a5.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
<li><p>学习速率调整，试图在每次更新过程中，
改变学习速率。从经验上看，<strong>学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。</strong>比较简单的学习率调整可以通过
<strong>学习率衰减（Learning Rate
Decay）</strong>的方式来实现。假设初始化学习率为 <span class="math inline">\(\eta_0\)</span>，在第 t 次迭代时的学习率 <span class="math inline">\(\eta_t\)</span>。常用的衰减方式为可以设置为
<strong>按迭代次数</strong> 进行衰减，迭代次数越大，学习率越小！ <img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/da9eec25972249f1a587a10d5723320f.jpeg" alt="image.png"></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/def103855dc34212bd5cfda1686a73bb.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
<li><p>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的，或者每个特征有着不同的统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f95707a6d51948c3a6aa7fcb61b5c055.jpg" alt="image.png"></p></li>
<li><p>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ab1758d6f7504c128eee9bdb71045e4d.png" alt="image.png"></p>
<p>简单的问题，一般使用随机梯度下降即可解决。在深度学习里，对梯度下降进行了很多改进，比如：自适应梯度下降。在深度学习章节，我们会具体介绍。</p></li>
<li><p>轮次和批次</p>
<p>轮次：epoch，轮次顾名思义是把我们已有的训练集数据学习多少轮，迭代多少次。</p>
<p>批次：batch，批次这里指的的我们已有的训练集数据比较多的时候，一轮要学习太多数据，
那就把一轮次要学习的数据分成多个批次，一批一批数据的学习。</p>
<p>就好比，你要背诵一片《赤壁赋》，很长。你在背诵的时候，一段段的背诵，就是批次batch。花费了一天终于背诵下来了，以后的9天，每天都进行一轮背诵复习，这就是轮次epoch。这样，《赤壁赋》的背诵效果，就非常牢固了。</p>
<p>在进行，机器学习训练时，我们也要合理选择轮次和批次~</p></li>
</ol>
<h3 id="代码实战梯度下降">3、代码实战梯度下降</h3>
<h4 id="批量梯度下降bgd-1">3.1、批量梯度下降BGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率，逆时衰减</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">1000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w)  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="随机梯度下降sgd-1">3.2、随机梯度下降SGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">6</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="小批量梯度下降mbgd-1">3.3、小批量梯度下降MBGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项 X_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-17T05:13:16.000Z" title="2023/8/17 13:13:16">2023-08-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-17T17:12:30.799Z" title="2023/8/18 01:12:30">2023-08-18</time></span><span class="level-item"><a class="link-muted" href="">机器学习</a></span><span class="level-item">39 minutes read (About 5870 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="../../2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">多元线性回归</a></p><div class="content"><meta name="referrer" content="no-referrer">
<h2 id="多元线性回归">多元线性回归</h2>
<h3 id="基本概念">1、基本概念</h3>
<p>​ 线性回归是机器学习中<strong>有监督</strong>机器学习下的一种算法。
<strong>回归问题</strong>主要关注的是<strong>因变量</strong>(需要预测的值，可以是一个也可以是多个)和一个或多个数值型的<strong>自变量</strong>(预测变量)之间的关系。</p>
<p>需要预测的值:即目标变量，target，y，<strong>连续值</strong>预测变量。</p>
<p>影响目标变量的因素：<span class="math inline">\(X_1\)</span>...<span class="math inline">\(X_n\)</span>，可以是连续值也可以是离散值。</p>
<p>因变量和自变量之间的关系:即<strong>模型</strong>，model，是我们要求解的。</p>
<h4 id="连续值">1.1、连续值</h4>
<p><img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//b.png" alt="b"> #### 1.2、离散值</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//a.png" alt="a">
<figcaption aria-hidden="true">a</figcaption>
</figure>
<h4 id="简单线性回归">1.3、简单线性回归</h4>
<p>前面提到过，算法说白了就是公式，简单线性回归属于一个算法，它所对应的公式。</p>
<p><span class="math display">\[y = wx + b\]</span></p>
<p>这个公式中，y 是目标变量即未来要预测的值，x 是影响 y 的因素，w,b
是公式上的参数即要求的模型。其实 b 就是咱们的截距，w 就是斜率嘛！
所以很明显如果模型求出来了，未来影响 y 值的未知数就是一个 x
值，也可以说影响 y 值
的因素只有一个，所以这是就叫<strong>简单</strong>线性回归的原因。</p>
<p>同时可以发现从 x 到 y 的计算，x
只是一次方，所以这是算法叫<strong>线性</strong>回归的原因。
其实，大家上小学时就已经会解这种一元一次方程了。为什么那个时候不叫人工智能算法呢？因为人工智能算法要求的是最优解！</p>
<h4 id="最优解">1.4、最优解</h4>
<p>Actual value:<strong>真实值</strong>，一般使用 y 表示。</p>
<p>Predicted value:<strong>预测值</strong>，是把已知的 x
带入到公式里面和<strong>猜</strong>出来的参数 w,b 计算得到的，一般使用
<span class="math inline">\(\hat{y}\)</span> 表示。</p>
<p>Error:<strong>误差</strong>，预测值和真实值的差距，一般使用 <span class="math inline">\(\varepsilon\)</span> 表示。</p>
<p><strong>最优解</strong>:尽可能的找到一个模型使得整体的误差最小，整体的误差通常叫做损失
Loss。</p>
<p>Loss:整体的误差，Loss 通过损失函数 Loss function 计算得到。</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//c.png" alt="c">
<figcaption aria-hidden="true">c</figcaption>
</figure>
<h4 id="多元线性回归-1">1.5、多元线性回归</h4>
<p>现实生活中，往往影响结果 y 的因素不止一个，这时 x 就从一个变成了 n
个，<span class="math inline">\(x_1\)</span>...<span class="math inline">\(x_n\)</span>
同时简单线性回归的公式也就不在适用了。<strong>多元线性回归</strong>公式如下：</p>
<p><span class="math inline">\(\hat{y} = w_1x_1 + w_2x_2 + …… + w_nx_n +
b\)</span></p>
<p>b是截距，也可以使用<span class="math inline">\(w_0\)</span>来表示</p>
<p><span class="math inline">\(\hat{y} = w_1x_1 + w_2x_2 + …… + w_nx_n +
w_0\)</span></p>
<p><span class="math inline">\(\hat{y} = w_1x_1 + w_2x_2 + …… + w_nx_n +
w_0 * 1\)</span></p>
<p>使用向量来表示，<span class="math inline">\(\vec{X}\)</span>表示所有的变量，是一维向量；<span class="math inline">\(\vec{W}\)</span>表示所有的系数（包含<span class="math inline">\(w_0\)</span>），是一维向量，根据向量乘法规律，可以这么写：</p>
<p><span class="math inline">\(\hat{y} =
W^TX\)</span>【默认情况下，向量都是列向量】</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//d.png" alt="d">
<figcaption aria-hidden="true">d</figcaption>
</figure>
<h3 id="正规方程">2、正规方程</h3>
<h4 id="最小二乘法矩阵表示">2.1、最小二乘法矩阵表示</h4>
<p><strong>最小二乘法</strong>可以将误差方程转化为有确定解的<strong>代数方程组</strong>（其方程式数目正好等于未知数的个数），从而可求解出这些未知参数。这个有确定解的代数方程组称为最小二乘法估计的<strong>正规方程</strong>。公式如下：</p>
<p><span class="math inline">\(\theta = (X^TX)^{-1}X^Ty\)</span> 或者
<span class="math inline">\(W = (X^TX)^{-1}X^Ty\)</span> ，其中的<span class="math inline">\(W、\theta\)</span> 即使方程的解！</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//e.png" alt="e">
<figcaption aria-hidden="true">e</figcaption>
</figure>
<p>公式是如何<strong>推导</strong>的？</p>
<p>最小二乘法公式如下：</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2}\sum\limits_{i =
0}^n(h_{\theta}(x_i) - y_i)^2\)</span></p>
<p>使用矩阵表示：</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//f.png" alt="f">
<figcaption aria-hidden="true">f</figcaption>
</figure>
<p>之所以要使用转置T，是因为，矩阵运算规律是：矩阵A的一行乘以矩阵B的一列！</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//g.png" alt="g">
<figcaption aria-hidden="true">g</figcaption>
</figure>
<h4 id="多元一次方程举例">2.2、多元一次方程举例</h4>
<p>1、二元一次方程</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//h.png" alt="h">
<figcaption aria-hidden="true">h</figcaption>
</figure>
<p>2、三元一次方程</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//i.png" alt="i">
<figcaption aria-hidden="true">i</figcaption>
</figure>
<p>3、八元一次方程</p>
<figure>
<img src="/2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92//j.png" alt="j">
<figcaption aria-hidden="true">j</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上面八元一次方程对应的X数据</span></span><br><span class="line">X = np.array([[  <span class="number">0</span> ,<span class="number">14</span> , <span class="number">8</span> ,  <span class="number">0</span> ,  <span class="number">5</span>,  -<span class="number">2</span>,   <span class="number">9</span>,  -<span class="number">3</span>],</span><br><span class="line"> [ -<span class="number">4</span> , <span class="number">10</span> ,  <span class="number">6</span> ,  <span class="number">4</span> ,-<span class="number">14</span> , -<span class="number">2</span> ,-<span class="number">14</span>  , <span class="number">8</span>],</span><br><span class="line"> [ -<span class="number">1</span> , -<span class="number">6</span>  , <span class="number">5</span> ,-<span class="number">12</span> ,  <span class="number">3</span> , -<span class="number">3</span> ,  <span class="number">2</span> , -<span class="number">2</span>],</span><br><span class="line"> [  <span class="number">5</span> , -<span class="number">2</span>  , <span class="number">3</span> , <span class="number">10</span>  , <span class="number">5</span> , <span class="number">11</span> ,  <span class="number">4</span>  ,-<span class="number">8</span>],</span><br><span class="line"> [-<span class="number">15</span> ,-<span class="number">15</span>  ,-<span class="number">8</span> ,-<span class="number">15</span> ,  <span class="number">7</span> , -<span class="number">4</span>, -<span class="number">12</span> ,  <span class="number">2</span>],</span><br><span class="line"> [ <span class="number">11</span> ,-<span class="number">10</span> , -<span class="number">2</span> ,  <span class="number">4</span>  , <span class="number">3</span> , -<span class="number">9</span> , -<span class="number">6</span> ,  <span class="number">7</span>],</span><br><span class="line"> [-<span class="number">14</span> ,  <span class="number">0</span> ,  <span class="number">4</span> , -<span class="number">3</span>  , <span class="number">5</span> , <span class="number">10</span> , <span class="number">13</span> ,  <span class="number">7</span>],</span><br><span class="line"> [ -<span class="number">3</span> , -<span class="number">7</span> , -<span class="number">2</span> , -<span class="number">8</span>  , <span class="number">0</span> , -<span class="number">6</span> , -<span class="number">5</span> , -<span class="number">9</span>]])</span><br><span class="line"><span class="comment"># 对应的y</span></span><br><span class="line">y = np.array([ <span class="number">339</span> ,-<span class="number">114</span>  , <span class="number">30</span> , <span class="number">126</span>, -<span class="number">395</span> , -<span class="number">87</span> , <span class="number">422</span>, -<span class="number">309</span>])</span><br><span class="line">display(X,y)</span><br></pre></td></tr></table></figure>
<h4 id="矩阵转置公式与求导公式">2.3、矩阵转置公式与求导公式：</h4>
<p><strong>转置公式如下：</strong></p>
<ul>
<li><span class="math inline">\((mA)^T = mA^T\)</span>，其中m是常数</li>
<li><span class="math inline">\((A + B)^T = A^T + B^T\)</span></li>
<li><span class="math inline">\((AB)^T = B^TA^T\)</span></li>
<li><span class="math inline">\((A^T)^T = A\)</span></li>
</ul>
<p><strong>求导公式如下：</strong></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/f8edeac442294ed5a1ab4a03f0de6cfe.png" alt="f8edeac442294ed5a1ab4a03f0de6cfe">
<figcaption aria-hidden="true">f8edeac442294ed5a1ab4a03f0de6cfe</figcaption>
</figure>
<h4 id="推导正规方程-theta-的解">2.4、推导正规方程 <span class="math inline">\(\theta\)</span> 的解：</h4>
<ol type="1">
<li><strong>矩阵乘法公式展开</strong></li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/77b1210b3bfa43cca35b0e3a79565c83.png" alt="77b1210b3bfa43cca35b0e3a79565c83">
<figcaption aria-hidden="true">77b1210b3bfa43cca35b0e3a79565c83</figcaption>
</figure>
<ol start="2" type="1">
<li><strong>进行求导（注意X、y是已知量，<span class="math inline">\(\theta\)</span> 是未知数）：</strong></li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/7a36e5110d1246a9b535921c2c943b16.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>根据<strong>2.3、矩阵转置公式与求导公式</strong>可知</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/c58118e586de408aa3ab7f5e28566a24.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="3" type="1">
<li><strong>根据上面求导公式进行运算：</strong></li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/81180de05e0e44fab748f77ec5b45365.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="4" type="1">
<li><strong>令导数<span class="math inline">\(J&#39;(\theta) =
0：\)</span></strong></li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/3934cd63cbf1444fb836cf3d5e70fe64.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="5" type="1">
<li><strong>矩阵没有除法，使用逆矩阵进行转化：</strong></li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/56641d424c9945a2a8452708cd8e27b3.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>到此为止，公式推导出来了~</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/98a7d8c20ba14fa1867937e75d1e29b7.gif" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="凸函数判定">2.5、凸函数判定</h4>
<p>判定损失函数是凸函数的好处在于我们可能很肯定的知道我们求得的极值即最优解，一定是全局最优解。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/fb6a9e9787304b58b096720379e9421f.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>如果是非凸函数，那就不一定可以获取全局最优解~</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/f60f1e2ea73148719cf19a255044a975.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>来一个更加立体的效果图：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/9822098f45bf4b4fa37eb859d361cce6.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>判定凸函数的方式:
判定凸函数的方式非常多，其中一个方法是看<strong>黑塞矩阵</strong>是否是<strong>半正定</strong>的。</p>
<p>黑塞矩阵(hessian matrix)是由目标函数在点 X
处的二阶偏导数组成的对称矩阵。</p>
<p>对于我们的式子来说就是在导函数的基础上再次对θ来求偏导，结果就是 <span class="math inline">\(X^TX\)</span>。所谓正定就是 <span class="math inline">\(X^TX\)</span> 的特征值全为正数，半正定就是 <span class="math inline">\(X^TX\)</span> 的特征值大于等于 0，
就是半正定。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/eda2806b9e5246e4bae439711efa5f55.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>这里我们对 <span class="math inline">\(J(\theta)\)</span>
损失函数求二阶导数的黑塞矩阵是 <span class="math inline">\(X^TX\)</span>
，得到的一定是半正定的，自己和自己做点乘嘛！</p>
<p>这里不用数学推导证明这一点。在机器学习中往往损失函数都是<strong>凸函数</strong>，到<strong>深度学习</strong>中损失函数往往是<strong>非凸函数</strong>，即找到的解<strong>未必</strong>是全局最优，只要模型堪用就好！机器学习特点是：不强调模型
100% 正确，只要是有价值的，堪用的，就Okay！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/b8d27b44461d447b8a0a4fda0f87f22f.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="线性回归算法推导">3、线性回归算法推导</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Soft_Po/article/details/118731058">最小二乘推导</a></p>
<h4 id="深入理解回归">3.1、深入理解回归</h4>
<p><strong>回归</strong>简单来说就是“回归平均值”(regression to the
mean)。但是这里的 mean 并不是把
历史数据直接当成未来的预测值，而是会把期望值当作预测值。
追根溯源<strong>回归</strong>这个词是一个叫高尔顿的人发明的，他通过大量观察数据发现:父亲比较高，儿子也比较高；父亲比较矮，那么儿子也比较矮！正所谓“龙生龙凤生凤老鼠的儿子会打洞”！但是会存在一定偏差~</p>
<p>父亲是 1.98，儿子肯定很高，但有可能不会达到1.98 父亲是
1.69，儿子肯定不高，但是有可能比 1.69 高</p>
<p>大自然让我们<strong>回归</strong>到一定的区间之内，这就是<strong>大自然神奇</strong>的力量。</p>
<p>高尔顿是谁？<strong>达尔文</strong>的表弟，这下可以相信他说的十有八九是<strong>对的</strong>了吧！</p>
<p>人类社会很多事情都被大自然这种神奇的力量只配置：身高、体重、智商、相貌……</p>
<p>这种神秘的力量就叫<strong>正态分布</strong>。大数学家高斯，深入研究了正态分布，最终推导出了线性回归的原理：<strong>最小二乘法</strong>！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/b187c8d76451404fb86d799444d0e950.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>接下来，我们跟着高斯的足迹继续向下走~</p>
<h4 id="误差分析">3.2、误差分析</h4>
<p>误差 <span class="math inline">\(\varepsilon_i\)</span> 等于第 i
个样本实际的值 <span class="math inline">\(y_i\)</span> 减去预测的值
<span class="math inline">\(\hat{y}\)</span> ，公式可以表达为如下：</p>
<p><span class="math inline">\(\varepsilon_i = |y_i -
\hat{y}|\)</span></p>
<p><span class="math inline">\(\varepsilon_i = |y_i -
W^Tx_i|\)</span></p>
<p>假定所有的样本的误差都是<strong>独立的</strong>，有上下的震荡，震荡认为是随机变量，足够多的随机变量叠加之后形成的分布，它服从的就是正态分布，因为它是正常状态下的分布，也就是高斯分布！<strong>均值</strong>是某一个值，<strong>方差</strong>是某一个值。
方差我们先不管，均值我们总有办法让它去等于零 0
的，因为我们这里是有截距b，
所有误差我们就可以认为是独立分布的，1&lt;=i&lt;=n，服从均值为
0，方差为某定值的<strong>高斯分布</strong>。机器学习中我们<strong>假设</strong>误差符合均值为0，方差为定值的正态分布！！！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/17e2809b4c85410e948cffe3643ad34d.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="最大似然估计">3.3、最大似然估计</h4>
<p>最大似然估计(maximum likelihood estimation,
MLE)一种重要而普遍的求估计量的方法。<strong>最大似然估计</strong>明确地使用概率模型，其目标是寻找能够以较高概率产生观察数据的系统发生树。最大似然估计是一类完全基于<strong>统计</strong>的系统发生树重建方法的代表。</p>
<p>是不是，有点看不懂，<strong>太学术</strong>了，我们举例说明~</p>
<p>假如有一个罐子，里面有<strong>黑白</strong>两种颜色的球，数目多少不知，两种颜色的<strong>比例</strong>也不知。我们想知道罐中白球和黑球的比例，但我们<strong>不能</strong>把罐中的球全部拿出来数。现在我们可以每次任意从已经<strong>摇匀</strong>的罐中拿一个球出来，<strong>记录</strong>球的颜色，然后把拿出来的球再<strong>放回</strong>罐中。这个过程可以<strong>重复</strong>，我们可以用记录的球的颜色来估计罐中黑白球的比例。假如在前面的一百次重复记录中，有七十次是白球，请问罐中白球所占的比例<strong>最有可能</strong>是多少？</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/5c0148f54cc344b6baec29a716498990.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>请告诉我答案！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/b5c23fbbefd84607abd1c614c68313e1.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>很多小伙伴，甚至不用算，凭感觉，就能给出答案：<strong>70%</strong>！</p>
<p><strong>下面是详细推导过程：</strong></p>
<ul>
<li><p>最大似然估计，计算</p></li>
<li><p>白球概率是p，黑球是1-p（罐子中非黑即白）</p></li>
<li><p>罐子中取一个请问是白球的概率是多少？</p>
<p><span class="math display">\[
p
\]</span></p></li>
<li><p>罐子中取两个球，两个球都是白色，概率是多少？</p>
<p><span class="math display">\[
p^2
\]</span></p></li>
<li><p>罐子中取5个球都是白色，概率是多少？</p>
<p><span class="math display">\[
p^5
\]</span></p></li>
<li><p>罐子中取10个球，9个是白色，一个是黑色，概率是多少呢？</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/71a9684cd4564aef96591431bbaa1a27.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li><span class="math inline">\(C_{10}^1 = C_{10}^1\)</span>
这个两个排列组合公式是<strong>相等的</strong>~</li>
<li><span class="math display">\[
C_{10}^9p^9(1-p) = C_{10}^1p^9(1-p)
\]</span></li>
</ul></li>
<li><p>罐子取100个球，70次是白球，30次是黑球，概率是多少？</p></li>
<li><p><span class="math display">\[
P = C_{100}^{30}p^{70}(1-p)^{30}
\]</span></p></li>
<li><p>最大似然估计，什么时候P最大呢？</p>
<p><span class="math inline">\(C_{100}^{30}\)</span>是常量，可以<strong>去掉</strong>！</p>
<p>p &gt; 0，1- p &gt;
0，所以上面概率想要求最大值，那么求<strong>导数</strong>即可！</p></li>
<li><p><span class="math display">\[
P&#39; = 70*p^{69}*(1-p)^{30} + p^{70}*30*(1-p)^{29}*(-1)
\]</span></p>
<p><strong>令导数为0：</strong></p></li>
<li><p><span class="math display">\[
0 = 70*p^{69}*(1-p)^{30} +p^{70}*30*(1-p)^{29}*(-1)
\]</span></p>
<p><strong>公式化简：</strong></p></li>
<li><p><span class="math display">\[
0 = 70*(1-p) - p*30
\]</span></p></li>
<li><p><span class="math display">\[
0 = 70 - 100*p
\]</span></p></li>
<li><p><strong>p = 70%</strong></p></li>
</ul>
<h4 id="高斯分布-概率密度函数">3.4、高斯分布-概率密度函数</h4>
<p>最常见的连续概率分布是<strong>正态分布</strong>，也叫<strong>高斯分布</strong>，而这正是我们所需要的，其概率密度函数如下:</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/f6bddf6943174fc88718cdad4cc3ce7e.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/4b353b48bba34cf9820ddcad0ef9548b.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>随着参数μ和σ<strong>变化</strong>，概率分布也产生变化。
下面重要的步骤来了，我们要把一组数据误差出现的<strong>总似然</strong>，也就是一组数据之所以对应误差出现的<strong>整体可能性</strong>表达出来了，因为数据的误差我们假设服从一个高斯分布，并且通过<strong>截距</strong>项来平移整体分布的位置从而使得<strong>μ=0</strong>，所以样本的误差我们可以表达其概率密度函数的值如下:</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/c1e8d58d707c4d65ba03cc873204f8d3.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>简化</strong>如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/8270a090b4fa4cbeb6cdc7214211c213.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="误差总似然">3.5、误差总似然</h4>
<p>和前面黑球白球问题<strong>类似</strong>，也是一个<strong>累乘</strong>问题~</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/d9636ac44e0f48fdac68b382c4f219c0.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>根据前面公式<span class="math inline">\(\varepsilon_i = |y_i -
W^Tx_i|\)</span>可以推导出来如下公式：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/f1b113438f234395b9524456a89c7006.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>公式中的<strong>未知变量</strong>就是<span class="math inline">\(W^T\)</span>，即方程的系数，系数包含截距~如果，把上面当成一个方程，就是概率P关于W的方程！其余符号，都是常量！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/1a4d510c461b478bac25f6abdbc68e5e.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>现在问题，就变换成了，求<strong>最大似然</strong>问题了！不过，等等~</p>
<p>累乘的最大似然，求解是非常麻烦的！</p>
<p>接下来，我们通过，求<strong>对数</strong>把<strong>累乘</strong>问题，转变为<strong>累加</strong>问题（加法问题，无论多复杂，都难不倒我了！）</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/a9518e4542fa44668535a7e33809239c.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="最小二乘法mse">3.6、最小二乘法MSE</h4>
<p><span class="math inline">\(P_W = \prod\limits_{i =
0}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i -
W^Tx_i)^2}{2\sigma^2}}\)</span></p>
<p>根据对数，单调性，对上面公式求自然底数e的对数，效果不变~</p>
<p><span class="math inline">\(log_e(P_W) = log_e(\prod\limits_{i =
0}^{n}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i -
W^Tx_i)^2}{2\sigma^2}})\)</span></p>
<p>接下来 log
函数继续为你带来惊喜，数学上连乘是个大麻烦，即使交给计算机去求解它也得<strong>哭出声来</strong>。惊喜是:</p>
<ul>
<li><span class="math inline">\(log_a(XY) = log_aX +
log_aY\)</span></li>
<li><span class="math inline">\(log_a\frac{X}{Y} = log_aX -
log_aY\)</span></li>
<li><span class="math inline">\(log_aX^n = n*log_aX\)</span></li>
<li><span class="math inline">\(log_a(X_1X_2……X_n) = log_aX_1 + log_aX_2
+ …… + log_aX_n\)</span></li>
<li><span class="math inline">\(log_xx^n = n(n\in R)\)</span></li>
<li><span class="math inline">\(log_a\frac{1}{X} = -log_aX\)</span></li>
<li><span class="math inline">\(log_a\sqrt[x]{N^y} =
\frac{y}{x}log_aN\)</span></li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/5a0de92b4e974103b61936748a72bd88.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>乘风破浪，继续推导---&gt;</strong></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/10425befead44d74a162d56db3586232.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>上面公式是最大似然求对数后的变形，其中<span class="math inline">\(\pi、\sigma\)</span>都是常量，而<span class="math inline">\((y_i -
W^Tx_i)^2\)</span>肯定大于<strong>零</strong>！上面求最大值问题，即可转变为如下求<strong>最小值</strong>问题：</p>
<p><span class="math inline">\(L(W) = \frac{1}{2}\sum\limits_{i =
0}^n(y^{(i)} - W^Tx^{(i)})^2\)</span>
L代表Loss，表示损失函数，损失函数<strong>越小</strong>，那么上面最大似然就<strong>越大</strong>~</p>
<p>有的书本上公式，也可以这样写，用<span class="math inline">\(J(\theta)\)</span>表示一个意思，<span class="math inline">\(\theta\)</span> 的角色就是W：</p>
<p><span class="math inline">\(J(\theta) = \frac{1}{2}\sum\limits_{i =
1}^n(y^{(i)} - \theta^Tx^{(i)})^2 = \frac{1}{2}\sum\limits_{i =
1}^n(\theta^Tx^{(i)} - y^{(i)})^2\)</span></p>
<p><strong>进一步提取：</strong></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/9cd20d0c4c8548e0be72f2e5899f24cc.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中：</p>
<p><span class="math inline">\(\hat{y} = h_{\theta}(X) =X
\theta\)</span>
表示全部数据，是矩阵，X表示多个数据，进行矩阵乘法时，放在前面</p>
<p><span class="math inline">\(\hat{y}_i = h_{\theta}(x^{(i)}) =
\theta^Tx^{(i)}\)</span>
表示第i个数据，是向量，所以进行乘法时，其中一方需要转置</p>
<p>因为最大似然公式中有个<strong>负号</strong>，所以最大总似然变成了<strong>最小化</strong>负号后面的部分。
到这里，我们就已经推导出来了 MSE 损失函数<span class="math inline">\(J(\theta)\)</span>，从公式我们也可以看出来 MSE
名字的来 历，mean squared error，上式也叫做最小二乘法！</p>
<h4 id="归纳总结升华">3.7、归纳总结升华</h4>
<p>这种最小二乘法估计，其实我们就可以认为，假定了误差服从正太分布，认为样本误差的出现是随机的，独立的，使用最大似然估计思想，利用损失函数最小化
MSE
就能求出最优解！所以反过来说，如果我们的数据误差不是互相独立的，或者不是随机出现的，那么就不适合去假设为正太分布，就不能去用正太分布的概率密度函数带入到总似然的函数中，故而就不能用
MSE 作为损失函数去求解最优解了！所以，最小二乘法不是万能的~</p>
<p>还有譬如假设误差服从泊松分布，或其他分布那就得用其他分布的概率密度函数去推导出损失函数了。</p>
<p>所以有时我们也可以把线性回归看成是广义线性回归。比如，逻辑回归，泊松回归都属于广义线性回归的一种，这里我们线性回归可以说是最小二乘线性回归。</p>
<h3 id="线性回归实战">4、线性回归实战</h3>
<h4 id="使用正规方程进行求解">4.1、使用正规方程进行求解</h4>
<h5 id="简单线性回归-1">4.1.1、简单线性回归</h5>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/739367b9a160496c9545b6d3aa157527.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>一元一次方程，在机器学习中一元表示一个特征，b表示截距，y表示目标值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 转化成矩阵</span></span><br><span class="line">X = np.linspace(<span class="number">0</span>,<span class="number">10</span>,num = <span class="number">30</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 斜率和截距，随机生成</span></span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">5</span>,size = <span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 根据一元一次方程计算目标值y，并加上“噪声”，数据有上下波动~</span></span><br><span class="line">y = X * w + b + np.random.randn(<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"><span class="comment"># 重新构造X，b截距，相当于系数w0，前面统一乘以1</span></span><br><span class="line">X = np.concatenate([X,np.full(shape = (<span class="number">30</span>,<span class="number">1</span>),fill_value= <span class="number">1</span>)],axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 正规方程求解</span></span><br><span class="line">θ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一元一次方程真实的斜率和截距是：&#x27;</span>,w, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;通过正规方程求解的斜率和截距是：&#x27;</span>,θ)</span><br><span class="line"><span class="comment"># 根据求解的斜率和截距绘制线性回归线型图</span></span><br><span class="line">plt.plot(X[:,<span class="number">0</span>],X.dot(θ),color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>效果如下（random.randn是随机生成正太分布数据，所以每次执行图形会有所不同）：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/e8786d05d012485ea92c2f26bd67679e.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h5 id="多元线性回归-2">4.1.2、多元线性回归</h5>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/59b917be60864200a61f60b3388cca1d.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>二元一次方程，<span class="math inline">\(x_1、x_2\)</span>
相当于两个特征，b是方程截距</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d.axes3d <span class="keyword">import</span> Axes3D <span class="comment"># 绘制三维图像</span></span><br><span class="line"><span class="comment"># 转化成矩阵</span></span><br><span class="line">x1 = np.random.randint(-<span class="number">150</span>,<span class="number">150</span>,size = (<span class="number">300</span>,<span class="number">1</span>))</span><br><span class="line">x2 = np.random.randint(<span class="number">0</span>,<span class="number">300</span>,size = (<span class="number">300</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 斜率和截距，随机生成</span></span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">5</span>,size = <span class="number">2</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 根据二元一次方程计算目标值y，并加上“噪声”，数据有上下波动~</span></span><br><span class="line">y = x1 * w[<span class="number">0</span>] + x2 * w[<span class="number">1</span>] + b + np.random.randn(<span class="number">300</span>,<span class="number">1</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.scatter(x1,x2,y) <span class="comment"># 三维散点图</span></span><br><span class="line">ax.view_init(elev=<span class="number">10</span>, azim=-<span class="number">20</span>) <span class="comment"># 调整视角</span></span><br><span class="line"><span class="comment"># 重新构造X，将x1、x2以及截距b，相当于系数w0，前面统一乘以1进行数据合并</span></span><br><span class="line">X = np.concatenate([x1,x2,np.full(shape = (<span class="number">300</span>,<span class="number">1</span>),fill_value=<span class="number">1</span>)],axis = <span class="number">1</span>)</span><br><span class="line">w = np.concatenate([w,b])</span><br><span class="line"><span class="comment"># 正规方程求解</span></span><br><span class="line">θ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y).<span class="built_in">round</span>(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;二元一次方程真实的斜率和截距是：&#x27;</span>,w)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;通过正规方程求解的斜率和截距是：&#x27;</span>,θ.reshape(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># # 根据求解的斜率和截距绘制线性回归线型图</span></span><br><span class="line">x = np.linspace(-<span class="number">150</span>,<span class="number">150</span>,<span class="number">100</span>)</span><br><span class="line">y = np.linspace(<span class="number">0</span>,<span class="number">300</span>,<span class="number">100</span>)</span><br><span class="line">z = x * θ[<span class="number">0</span>] + y * θ[<span class="number">1</span>] + θ[<span class="number">2</span>]</span><br><span class="line">ax.plot(x,y,z ,color = <span class="string">&#x27;red&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/b7615df8b1624df78b5168fc58a66475.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="机器学习库scikit-learn">4.2、机器学习库scikit-learn</h4>
<h5 id="scikit-learn简介">4.2.1、<a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/index.html">scikit-learn简介</a></h5>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/e813979dfefc4f0caaed58f922d6d587.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h5 id="scikit-learn实现简单线性回归">4.2.2、scikit-learn实现简单线性回归</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 转化成矩阵</span></span><br><span class="line">X = np.linspace(<span class="number">0</span>,<span class="number">10</span>,num = <span class="number">30</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 斜率和截距，随机生成</span></span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">5</span>,size = <span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 根据一元一次方程计算目标值y，并加上“噪声”，数据有上下波动~</span></span><br><span class="line">y = X * w + b + np.random.randn(<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"><span class="comment"># 使用scikit-learn中的线性回归求解</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X,y)</span><br><span class="line">w_ = model.coef_</span><br><span class="line">b_ = model.intercept_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一元一次方程真实的斜率和截距是：&#x27;</span>,w, b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;通过scikit-learn求解的斜率和截距是：&#x27;</span>,w_,b_)</span><br><span class="line">plt.plot(X,X.dot(w_) + b_,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/f06d7439704f4d2a95254d6be0222b36.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h5 id="scikit-learn实现多元线性回归">4.2.3、scikit-learn实现多元线性回归</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d.axes3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="comment"># 转化成矩阵</span></span><br><span class="line">x1 = np.random.randint(-<span class="number">150</span>,<span class="number">150</span>,size = (<span class="number">300</span>,<span class="number">1</span>))</span><br><span class="line">x2 = np.random.randint(<span class="number">0</span>,<span class="number">300</span>,size = (<span class="number">300</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 斜率和截距，随机生成</span></span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">5</span>,size = <span class="number">2</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 根据二元一次方程计算目标值y，并加上“噪声”，数据有上下波动~</span></span><br><span class="line">y = x1 * w[<span class="number">0</span>] + x2 * w[<span class="number">1</span>] + b + np.random.randn(<span class="number">300</span>,<span class="number">1</span>)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">ax.scatter(x1,x2,y) <span class="comment"># 三维散点图</span></span><br><span class="line">ax.view_init(elev=<span class="number">10</span>, azim=-<span class="number">20</span>) <span class="comment"># 调整视角</span></span><br><span class="line"><span class="comment"># 重新构造X，将x1、x2以及截距b，相当于系数w0，前面统一乘以1进行数据合并</span></span><br><span class="line">X = np.concatenate([x1,x2],axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 使用scikit-learn中的线性回归求解</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X,y)</span><br><span class="line">w_ = model.coef_.reshape(-<span class="number">1</span>)</span><br><span class="line">b_ = model.intercept_</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;二元一次方程真实的斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;通过scikit-learn求解的斜率和截距是：&#x27;</span>,w_,b_)</span><br><span class="line"><span class="comment"># # 根据求解的斜率和截距绘制线性回归线型图</span></span><br><span class="line">x = np.linspace(-<span class="number">150</span>,<span class="number">150</span>,<span class="number">100</span>)</span><br><span class="line">y = np.linspace(<span class="number">0</span>,<span class="number">300</span>,<span class="number">100</span>)</span><br><span class="line">z = x * w_[<span class="number">0</span>] + y * w_[<span class="number">1</span>] + b_</span><br><span class="line">ax.plot(x,y,z ,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1651232743056/0066efeb6e5e4bfbaba999022d3009bd.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-16T17:30:16.000Z" title="2023/8/17 01:30:16">2023-08-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-17T17:48:53.577Z" title="2023/8/18 01:48:53">2023-08-18</time></span><span class="level-item"><a class="link-muted" href="">机器学习</a></span><span class="level-item">an hour read (About 9832 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="../../2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96/">梯度下降优化</a></p><div class="content"><meta name="referrer" content="no-referrer">
<h2 id="梯度下降优化">梯度下降优化</h2>
<h3 id="归一化-normalization">1、归一化 Normalization</h3>
<h4 id="归一化目的">1.1、归一化目的</h4>
<p>  梯度下降的原理和应用，我们已经在前面课程中进行了学习，大家仔细观察下图。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/c361fdfa8946471c97c40e111bc4c17d.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  不同方向的<strong>陡峭度</strong>是不一样的，即不同维度的数值大小是不同。也就是说梯度下降的快慢是不同的：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/83a1603692cc44d4ab90672dc31b2fde.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p> 如果维度多了，就是<strong>超平面</strong>（了解一下霍金所说的宇宙十一维空间），很难画出来了，感受一下下面这张图的空间维度情况。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/60843367a65a49dcb8f61bd689aa7e68.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  如果拿多元线性回归举例的话，因为多元线性回归的损失函数 MSE
是凸函数，所以我们可以把损失函数看成是一个碗。然后下面的图就是从碗上方去俯瞰！哪里是损失最小的地方呢？当然对应的就是碗底的地方！所以下图碗中心的地方颜色较浅的区域就是损失函数最小的地方。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/8577307ca13a44b29a344ed49bde7704.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  上面两张图都是进行梯度下降，你有没有发现，略有不同啊？两张图形都是鸟瞰图，左边的图形做了归一化处理，右边是没有做归一化的俯瞰图。</p>
<p>  啥是归一化呢？请带着疑问跟我走~</p>
<p>  我们先来说一下为什么没做归一化是右侧图示，举个例子假如我们客户数据信息，有两个维度，一个是用户的年龄，一个是用户的月收入，目标变量是快乐程度。</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">name</th>
<th style="text-align: center;">age</th>
<th style="text-align: center;">salary</th>
<th style="text-align: center;">happy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">路博通</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">7000</td>
<td style="text-align: center;">100</td>
</tr>
<tr class="even">
<td style="text-align: center;">马老师</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">180</td>
</tr>
<tr class="odd">
<td style="text-align: center;">赵老师</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">30000</td>
<td style="text-align: center;">164</td>
</tr>
<tr class="even">
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
</tr>
</tbody>
</table>
<p>  我们可以里面写出线性回归公式， <span class="math inline">\(y =
\theta_1x_1 + \theta_2x_2 + b\)</span>
，那么这样每一条样本不同维度对应的数量级不同，原因是每个维度对应的物理含义不同嘛，但是计算机能理解
36 和 7000 分别是年龄和收入吗？计算机只是拿到一堆数字而已。</p>
<p>  我们把 <span class="math inline">\(x_1\)</span> 看成是年龄，<span class="math inline">\(x_2\)</span> 看成是收入， y
对应着快乐程度。机器学习就是在知道
X，y的情况下解方程组调整出最优解的过程。根据公式我们也可以发现 y
是两部分贡献之和，按常理来说，一开始并不知道两个部分谁更重要的情况下，可以想象为两部分对
y 的贡献是一样的即 <span class="math inline">\(\theta_1x_1 =
\theta_2x_2\)</span> ，如果 <span class="math inline">\(x_1 \ll
x_2\)</span> ，那么最终 <span class="math inline">\(\theta_1 \gg
\theta_2\)</span> （远大于）。</p>
<p>  这样是不是就比较好理解为什么之前右侧示图里为什么 <span class="math inline">\(\theta_1 &gt; \theta_2\)</span>
，看起来就是椭圆。再思考一下，梯度下降第 1 步的操作，是不是所有的维度
<span class="math inline">\(\theta\)</span> 都是根据在期望 <span class="math inline">\(\mu\)</span> 为 0 方差 <span class="math inline">\(\sigma\)</span> 为 1
的正太分布随机生成的，说白了就是一开始的 <span class="math inline">\(\theta_1\)</span> 和 <span class="math inline">\(\theta_2\)</span> 数值是差不多的。所以可以发现
<span class="math inline">\(\theta_1\)</span> 从初始值到目标位置 <span class="math inline">\(\theta_1^{target}\)</span> 的距离要远大于 <span class="math inline">\(\theta_2\)</span> 从初始值到目标位置<span class="math inline">\(\theta_2^{target}\)</span>。</p>
<p>  因为 <span class="math inline">\(x_1 \ll x_2\)</span>，根据梯度公式
<span class="math inline">\(g_j= (h_{\theta}(x) - y)x_j\)</span> ，得出
<span class="math inline">\(g_1 \ll
g_2\)</span>。根据梯度下降公式：<span class="math inline">\(\theta_j^{n+1} = \theta_j^n - \eta * g_j\)</span>
可知，每次调整 <span class="math inline">\(\theta_1\)</span> 的幅度
<span class="math inline">\(\ll\)</span> （远小于） <span class="math inline">\(\theta_2\)</span> 的调整幅度。</p>
<p>  总结一下 ，根据上面得到的两个结论 ，它俩之间是互相矛盾的
，意味着最后 <span class="math inline">\(\theta_2\)</span> 需要比 <span class="math inline">\(\theta_1\)</span>
更少的迭代次数就可以收敛，而我们要最终求得最优解，就必须每个维度 <span class="math inline">\(\theta\)</span> 都收敛才可以，所以会出现 <span class="math inline">\(\theta_2\)</span> 等待 <span class="math inline">\(\theta_1\)</span>
收敛的情况。讲到这里对应图大家应该可以理解为什么右图是先顺着 <span class="math inline">\(\theta_2\)</span>
的坐标轴往下走再往右走的原因了吧。</p>
<p><strong>结论:</strong></p>
<p>  归一化的一个目的是，使得梯度下降在不同维度 <span class="math inline">\(\theta\)</span>
参数（不同数量级）上，可以步调一致协同的进行梯度下降。这就好比社会主义，一小部分人先富裕起来了，先富带后富，这需要一定的时间，先富的这批人等待其他的人富裕起来；但是，更好途经是实现共同富裕，最后每个人都不能落下，
优化的步伐是一致的。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/46723e3f15054b108136b59fa0836252.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>经过归一化处理，收敛的速度，明显快了！</p>
<h4 id="归一化本质">1.2、归一化本质</h4>
<p>  做归一化的目的是要实现<strong>共同富裕</strong>，而之所以梯度下降优化时不能达到步调一致的根本原因其实还是
<span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 的数量级不同。所以什么是归一化？</p>
<p>  答案自然就出来了，就是把 <span class="math inline">\(x_1\)</span>
和 <span class="math inline">\(x_2\)</span>
的数量级统一，扩展一点说，如果有更多特征维度，就要把各个特征维度 <span class="math inline">\(x_1、x_2、……、x_n\)</span>
的数量级统一，来做到无量纲化。</p>
<h4 id="最大值最小值归一化">1.3、最大值最小值归一化</h4>
<p>  也称为离差标准化，是对原始数据的线性变换，<strong>使结果值映射到[0
- 1]之间</strong>。转换函数如下：</p>
<p><span class="math inline">\(X^* = \frac{X - X\_min}{X\_max
-X\_min}\)</span></p>
<p>  其实我们很容易发现使用最大值最小值归一化（min-max标准化）的时候，优点是一定可以把数值归一到
0 ~ 1
之间，缺点是如果有一个离群值（比如马云的财富），正如我们举的例子一样，会使得一个数值为
1，其它数值都几乎为 0，所以受离群值的影响比较大！</p>
<p><strong>代码演示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">x_ = (x - x.<span class="built_in">min</span>(axis = <span class="number">0</span>)) / (x.<span class="built_in">max</span>(axis = <span class="number">0</span>) - x.<span class="built_in">min</span>(axis = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p><strong>使用scikit-learn函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">min_max_scaler = MinMaxScaler()</span><br><span class="line">x_ = min_max_scaler.fit_transform(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<h4 id="均值标准化">1.4、0-均值标准化</h4>
<p>  这种方法给予原始数据的均值（mean）和标准差（standard
deviation）进行数据的标准化，也叫做Z-score标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：</p>
<p><span class="math inline">\(X^* = \frac{X - \mu}{\sigma}\)</span></p>
<p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p>
<p><span class="math inline">\(\mu = \frac{1}{n}\sum\limits_{i =
1}^nx_i\)</span></p>
<p><span class="math inline">\(\sigma = \sqrt{\frac{1}{n}\sum\limits_{i
= 1}^n(x_i - \mu)^2}\)</span></p>
<p>  相对于最大值最小值归一化来说，因为标准归一化除以了标准差，而标准差的计算会考虑到所有样本数据，所以受到离群值的影响会小一些，这就是除以方差的好处！但是，0-均值标准化不一定会把数据缩放到
0 ~ 1 之间了。既然是0均值，也就意味着，有正有负！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">x_ = (x - x.mean(axis = <span class="number">0</span>)) / x.std(axis = <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p><strong>使用scikit-learn函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">standard_scaler = StandardScaler()</span><br><span class="line">x_ = standard_scaler.fit_transform(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p>  那为什么要减去均值呢？其实做均值归一化还有一个特殊的好处（对比最大值最小值归一化，全部是正数0~1），我们来看一下梯度下降的式子，你就会发现
<span class="math inline">\(\alpha\)</span> 是正数，不管 A 是正还是负（
A 就是 <span class="math inline">\(\hat{y} - y = h_{\theta}(x) -
y\)</span>），对于所有的维度 X，比如这里的 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 来说，<span class="math inline">\(\alpha\)</span> 乘上 A
都是一样的符号，那么每次迭代的时候 <span class="math inline">\(w_1^{t+1}\)</span> 和 <span class="math inline">\(w_2^{t+1}\)</span>
的更新幅度符号也必然是一样的，这样就会像下图有右侧所示：要想从 <span class="math inline">\(w_t\)</span> 更新到 <span class="math inline">\(w^*\)</span> 就必然要么 <span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span> 同时变大再同时变小，或者就 <span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span>
同时变小再同时变大。不能如图上所示蓝色的最优解路径，即 <span class="math inline">\(w_1\)</span> 变小的同时 <span class="math inline">\(w_2\)</span> 变大！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/2cdaf2f432bb4e419ed2e690197df8b6.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  那我们如何才能做到让 <span class="math inline">\(w_1\)</span>
变小的时候 <span class="math inline">\(w_2\)</span>
变大呢？归其根本还是数据集 X
矩阵（经过min-max归一化）中的数据均为正数。所以如果我们可以让 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span>
它们符号不同，比如有正有负，其实就可以在做梯度下降的时候有更多的可能性去让更新尽可能沿着最优解路径去走。</p>
<p>  结论：<strong>0-均值标准化</strong>处理数据之后，属性有正有负，可以让梯度下降沿着最优路径进行~</p>
<p><strong>注意：</strong></p>
<p>  我们在做特征工程的时候，很多时候如果对训练集的数据进行了预处理，比如这里讲的归一化，那么未来对测试集的时候，和模型上线来新的数据的时候，都要进行<strong>相同的</strong>数据预处理流程，而且所使用的均值和方差是来自当时训练集的均值和方差!</p>
<p>  因为我们人工智能要干的事情就是从训练集数据中找规律，然后利用找到的规律去预测新产生的数据。这也就是说假设训练集和测试集以及未来新来的数据是属于同分布的！从代码上面来说如何去使用训练集的均值和方差呢？就需要把
scaler 对象持久化，
回头模型上线的时候再加载进来去对新来的数据进行处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line">joblib.dump(standard_scaler,<span class="string">&#x27;scale&#x27;</span>) <span class="comment"># 持久化</span></span><br><span class="line">standard_scaler = joblib.load(<span class="string">&#x27;scale&#x27;</span>) <span class="comment"># 加载</span></span><br><span class="line">standard_scaler.transform(x) <span class="comment"># 使用</span></span><br></pre></td></tr></table></figure>
<h3 id="正则化-regularization">2、正则化 Regularization</h3>
<h4 id="过拟合欠拟合">2.1、过拟合欠拟合</h4>
<ol type="1">
<li>欠拟合（under
fit）：还没有拟合到位，训练集和测试集的准确率都还没有到达最高，学的还不到位。</li>
<li>过拟合（over
fit）：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了（走火入魔），做过的卷子都能再次答对（死记硬背），考试碰到新的没见过的题就考不好（不会举一反三）。</li>
<li>恰到好处（just
right）：过拟合前，训练集和测试集准确率都达到巅峰。好比，学习并不需要花费很多时间，理解的很好，考试的时候可以很好的把知识举一反三。</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/793562d28f114fa5924bbcd3f6361786.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  正则化就是防止过拟合，增加模型的<strong>鲁棒性</strong>，鲁棒是
Robust
的音译，也就是强壮的意思。就像计算机软件在面临攻击、网络过载等情况下能够不死机不崩溃，这就是软件的鲁棒性。鲁棒性调优就是让模型拥有更好的鲁棒性，也就是让模型的泛化能力和推广
能力更加的强大。</p>
<p>  举例子说明：下面两个式子描述同一条直线那个更好？</p>
<p><span class="math inline">\(y = 0.3x_1 + 0.4x_2 + 0.5\)</span></p>
<p><span class="math inline">\(y = 3x_1 + 4x_2 + 5\)</span></p>
<p>  第一个更好，因为下面的公式是上面的十倍，当 w
越小公式的容错的能力就越好。因为把测试数据带入公式中如果测试集原来是
[32, 128] 在带入的时候发生了一些偏差，比如说变成 [30, 120]
，第二个模型结果就会比第一个模型结果的偏差大的多。公式中 <span class="math inline">\(y = W^Tx\)</span> ，当 x
有一点错误，这个错误会通过 w 放大。但是 w 不能太小，当 w
太小时（比如都趋近0），模型就没有意义了，无法应用。想要有一定的容错率又要保证正确率就要由正则项来发挥作用了！</p>
<p>  所以正则化(鲁棒性调优)的本质就是牺牲模型在训练集上的正确率来提高推广、泛化能力，
W
在数值上越小越好，这样能抵抗数值的<strong>扰动</strong>。同时为了保证模型的正确率
W 又不能极小。
故而人们将原来的损失函数加上一个惩罚项，这里面损失函数就是原来固有的损失函数，比如回归的话通常是
MSE，分类的话通常是 cross entropy
交叉熵，然后在加上一部分惩罚项来使得计算出来的模型 W
相对小一些来带来泛化能力。</p>
<p>  常用的惩罚项有L1 正则项或者 L2 正则项：</p>
<ul>
<li><span class="math inline">\(L_1 = ||w||_1 = \sum\limits_{i =
1}^n|w_i|\)</span> 对应曼哈顿距离</li>
<li><span class="math inline">\(L_2 = ||w||_2 = \sqrt{\sum\limits_{i =
1}^n(w_i)^2}\)</span> 对应欧氏距离</li>
</ul>
<p>其实 L1 和 L2
正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离：</p>
<p><span class="math inline">\(L_p = ||X||_p = \sqrt[p]{\sum\limits_{i =
1}^nx_i^p} , X = (x_1,x_2,……x_n)\)</span></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/95727c537b1e41a39c1a698bae619d7d.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  当我们把多元线性回归损失函数加上 L2 正则的时候，就诞生了 Ridge
岭回归。当我们把多元线性回归损失函数加上 L1 正则的时候，就孕育出来了
Lasso 回归。其实 L1 和 L2
正则项惩罚项可以加到任何算法的损失函数上面去提高计算出来模型的泛化能力的。</p>
<h4 id="套索回归lasso">2.2、套索回归（Lasso）</h4>
<p>先从线性回归开始，其损失函数如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/3c449aa80d4d4b5cb02114ea1d347445.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L1正则化的损失函数，令<span class="math inline">\(J_0 =
J(\theta)\)</span>：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/bdb461ed1a6746258baf6fe48988627c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>令 <span class="math inline">\(L_1 = \alpha * \sum\limits_{i =
1}^n|w_i|\)</span> ：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ac47115c4dcb437f8c96c799530a142d.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  其中 <span class="math inline">\(J_0\)</span>
是原始的损失函数，加号后面的一项是L1正则化项， <span class="math inline">\(\alpha\)</span> 是正则化系数。注意到
L1正则化是权值的绝对值之和。<span class="math inline">\(J\)</span>
是带有绝对值符号的函数，因此 <span class="math inline">\(J\)</span>
是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数
<span class="math inline">\(J_0\)</span> 后面添加L1正则项时，相当于对
<span class="math inline">\(J_0\)</span> 做了一个约束。令<span class="math inline">\(L_1 = \alpha * \sum\limits_{i = 1}^n|w_i|\)</span>
，则 <span class="math inline">\(J = J_0 + L_1\)</span>
，此时我们的任务变成在 <span class="math inline">\(L_1\)</span>
约束下求出 <span class="math inline">\(J_0\)</span>
取最小值的解。<strong>考虑二维的情况</strong>，即只有两个权值 <span class="math inline">\(w_1、w_2\)</span> ，此时 <span class="math inline">\(L_1 = |w_1| + |w_2|\)</span>。
对于梯度下降法，求解 <span class="math inline">\(J_0\)</span>
过程可以画出等值线，同时 L1 正则化的函数 <span class="math inline">\(L_1\)</span> 也可以在 <span class="math inline">\(w_1、w_2\)</span>所在的平面上画出来：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ebbe4decd1e7445c9df9085f85b26049.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  图中等值线是<span class="math inline">\(J_0\)</span>的等值线，是椭圆形。黑色方框是 <span class="math inline">\(L_1\)</span> 函数的图形，<span class="math inline">\(L_1 = |w_1| + |w_2|\)</span>
这个函数画出来，就是一个方框。</p>
<p>  在图中，当 <span class="math inline">\(J_0\)</span> 等值线与 <span class="math inline">\(L_1\)</span> 图形首次相交的地方就是最优解。上图中
<span class="math inline">\(J_0\)</span> 与 <span class="math inline">\(L_1\)</span> 在 <span class="math inline">\(L_1\)</span>
的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是 <span class="math inline">\((w_1,w_2) = (0,w)\)</span> 。可以直观想象，因为
<span class="math inline">\(L_1\)</span>
函数有很多『突出的角』（二维情况下四个，多维情况下更多）， <span class="math inline">\(J_0\)</span> 与这些角接触的机率会远大于与 <span class="math inline">\(L_1\)</span>
其它部位接触的机率（这是很直觉的想象，突出的角比直线的边离等值线更近写），而在这些角上，会有很多权值等于0（因为角就在坐标轴上），这就是为什么
L1 正则化可以产生稀疏模型（很多权重等于0了），进而可以用于特征选择。</p>
<p>  而正则化前面的系数 <span class="math inline">\(\alpha\)</span>，可以控制 <span class="math inline">\(L_1\)</span> 图形的大小。<span class="math inline">\(\alpha\)</span> 越小，<span class="math inline">\(L_1\)</span> 的图形越大（上图中的黑色方框）；<span class="math inline">\(\alpha\)</span> 越大，<span class="math inline">\(L_1\)</span>
的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优解的值<span class="math inline">\((w_1,w_2) = (0,w)\)</span> 中的 w
可以取到很小的值的原因所在。</p>
<p>代码演示 <span class="math inline">\(\alpha\)</span>
取值大小对黑色方框的尺寸影响：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># α 的值是：1</span></span><br><span class="line"><span class="comment"># 1 = x + y</span></span><br><span class="line"><span class="comment"># y = 1 -x</span></span><br><span class="line">f = <span class="keyword">lambda</span> x : <span class="number">1</span>- x</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">plt.plot(x, f(x), color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># α 的值是：3</span></span><br><span class="line"><span class="comment"># 1 = 3 * x + 3 * y</span></span><br><span class="line"><span class="comment"># y = 1/3 -x</span></span><br><span class="line">f2 = <span class="keyword">lambda</span> x : <span class="number">1</span>/<span class="number">3</span> - x </span><br><span class="line">x2 = np.linspace(<span class="number">0</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line">plt.plot(x2, f2(x2),color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些列设置</span></span><br><span class="line">plt.xlim(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.ylim(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;None&#x27;</span>)  <span class="comment"># 将图片的右框隐藏</span></span><br><span class="line">ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;None&#x27;</span>)  <span class="comment"># 将图片的上边框隐藏</span></span><br><span class="line">ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>, <span class="number">0</span>)) <span class="comment"># x轴出现在y轴的-1 位置</span></span><br><span class="line">ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>, <span class="number">0</span>))</span><br><span class="line">plt.savefig(<span class="string">&#x27;./图片/13-alpha对方框影响.png&#x27;</span>,dpi = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/36f894d8be794c7fb0cfd6655acc1e85.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>权重更新规则如下：</strong></p>
<ol type="1">
<li>损失函数：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/873b8438cc4d46c39e88308f6fe63d6f.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="2" type="1">
<li>更新规则：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/deb751cc8eab47b0b620e2f8edae185c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(J_0\)</span>
即是线性回归的损失函数，<span class="math inline">\(L_1\)</span>
是添加的正则项。<span class="math inline">\(sgn(w_i)\)</span>
表示符号函数、指示函数，值为：1 或 -1。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/18bf2efda1204dd08081f1cdc9198a0a.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>注意当 <span class="math inline">\(w_i = 0\)</span> 时不可导。</p>
<p><strong>综上所述</strong>，L1正则化权重更新如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/05cfc081b4374d4aa32a3e76e5426f01.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li>Lasso回归和线性回归相比，多了一项：<span class="math inline">\(-\eta
* \alpha * sgn(w_i)\)</span></li>
<li>$$ 大于零，表示梯度下降学习率</li>
<li><span class="math inline">\(\alpha\)</span>
大于零，表示L1正则化系数</li>
<li>当<span class="math inline">\(w_i\)</span>为正时候 <span class="math inline">\(sgn(w_i) = 1\)</span>，直接减去 <span class="math inline">\(\eta * \alpha\)</span> （大于0），所以正的 <span class="math inline">\(w_i\)</span> 变小了</li>
<li>当<span class="math inline">\(w_i\)</span>为负时候 <span class="math inline">\(sgn(w_i) = -1\)</span>，相当于直接加上 <span class="math inline">\(\eta * \alpha\)</span> （大于0），所以负的 <span class="math inline">\(w_i\)</span> 变大了，绝对值变小，向0靠近</li>
</ul>
<p>有的书本上公式会这样写，其中 <span class="math inline">\(\lambda\)</span> 表示L1正则化系数：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/f271019eeee445a48346efbda1acca44.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="岭回归ridge">2.3、岭回归（Ridge）</h4>
<p>也是先从线性回归开始，其损失函数如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/1c5b0185a068410eb3f3c1a3f82f0c72.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L2正则化的损失函数（对L2范数，进行了平方运算），令<span class="math inline">\(J_0 = J(\theta)\)</span>：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/b844b823dfcc4437ae7afd0f1f0b680c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>令 <span class="math inline">\(L_2 = \alpha * \sum\limits_{i =
1}^n(w_i)^2\)</span> ：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/1cbddf3ce5da4e989c860fb392e93655.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样可以画出他们在二维平面上的图形，如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ed5a6039416f48c1932f6565919d67f7.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>二维平面下 L2
正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此
<span class="math inline">\(J_0\)</span> 与 <span class="math inline">\(L_2\)</span> 相交时使得 <span class="math inline">\(w_1、w_2\)</span>
等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数
w 都为0的情况（这种情况就叫稀疏性）！</p>
<p><strong>权重更新规则如下：</strong></p>
<ol type="1">
<li>损失函数：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/d13ef5f48f584dc28a629673647035a2.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="2" type="1">
<li>更新规则：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/3bc2481fb852443a98553e4e95ad670b.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(J_0\)</span>
即是线性回归的损失函数，<span class="math inline">\(L_2\)</span>
是添加的正则项。</p>
<p><strong>综上所述</strong>，L2正则化权重更新如下（$ 2$
也是常数项，可以合并到一起用整体 <span class="math inline">\(\alpha\)</span> 替代）：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/54a6992795ec49e8af1f268e3ab0f283.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(\alpha\)</span>
就是正则化参数，<span class="math inline">\(\eta\)</span>
表示学习率。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，
<span class="math inline">\(\theta_j\)</span>
都要先乘以一个小于1的因子（即 <span class="math inline">\((1-\eta *
\alpha)\)</span> ），从而使得 <span class="math inline">\(\theta_j\)</span> 加速减小，因此总的来看，<span class="math inline">\(\theta\)</span>
相比不加L2正则项的线性回归可以获得更小的值。从而，实现了防止过拟合的效果，增加模型的鲁棒性~</p>
<p>有的书本上，公式写法可能<strong>不同</strong>：其中 <span class="math inline">\(\lambda\)</span> 表示正则化参数。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/0910c26576bd4e159e46c0770caa1995.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="线性回归衍生算法">3、线性回归衍生算法</h3>
<p>  接下来，我们一起学习一下scikit-learn中为我们提供的线性回归衍生算法，根据上面所学的原理，对比线性回归加深理解。</p>
<h4 id="ridge算法使用">3.1、Ridge算法使用</h4>
<p>这是scikit-learn官网给出的岭回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/c16bea0c68cd4e749269f2b4d089d73f.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L2正则化和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">ridge = Ridge(alpha= <span class="number">1</span>, solver=<span class="string">&#x27;sag&#x27;</span>)</span><br><span class="line">ridge.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;岭回归求解的斜率：&#x27;</span>,ridge.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;岭回归求解的截距：&#x27;</span>,ridge.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>,l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知L2正则化，将方程系数进行了缩小</li>
<li><span class="math inline">\(\alpha\)</span>
增大求解出来的方程斜率变小</li>
<li>Ridge回归源码解析：
<ul>
<li>alpha：正则项系数</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>solver：优化算法的选择</li>
</ul></li>
</ul>
<h4 id="lasso算法使用">3.2、Lasso算法使用</h4>
<p>这是scikit-learn官网给出的套索回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/b3deb13a7a55485d82e4417549bb8633.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>公式中多了一项：<span class="math inline">\(\frac{1}{2n_{samples}}\)</span>这是一个常数项，去掉之后，也不会影响损失函数公式计算。在岭回归中，就没有这项。</p>
<p>L1正则化和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">20</span>)</span><br><span class="line">w = np.random.randn(<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">lasso = Lasso(alpha= <span class="number">0.5</span>)</span><br><span class="line">lasso.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;套索回归求解的斜率：&#x27;</span>,lasso.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;套索回归求解的截距：&#x27;</span>,lasso.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>, l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知L1正则化，将方程系数进行了缩减，部分系数为0，产生稀疏模型</li>
<li><span class="math inline">\(\alpha\)</span>
越大，模型稀疏性越强，越多的参数为0</li>
<li>Lasso回归源码解析：
<ul>
<li>alpha：正则项系数</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>precompute：bool
类型，默认值为False，决定是否提前计算Gram矩阵来加速计算</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练</li>
</ul></li>
</ul>
<h4 id="elastic-net算法使用">3.3、Elastic-Net算法使用</h4>
<p>这是scikit-learn官网给出的弹性网络回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/91558aeaf118405085a6f799c1e2bcb4.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  Elastic-Net 回归，即岭回归和Lasso技术的混合。弹性网络是一种使用
L1， L2 范数作为先验正则项训练的线性回归模型。
这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso
一样，但是它仍然保持一些像 Ridge 的正则性质。我们可利用 l1_ratio
参数控制 L1 和 L2 的凸组合。</p>
<p>  弹性网络在很多特征互相联系（相关性，比如<strong>身高</strong>和<strong>体重</strong>就很有关系）的情况下是非常有用的。Lasso
很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p>  在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在迭代过程中继承
Ridge 的稳定性。</p>
<p>弹性网络回归和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">20</span>)</span><br><span class="line">w = np.random.randn(<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">model = ElasticNet(alpha= <span class="number">1</span>, l1_ratio = <span class="number">0.7</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;弹性网络回归求解的斜率：&#x27;</span>,model.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;弹性网络回归求解的截距：&#x27;</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>, l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知Elastic-Net网络模型，融合了L1正则化L2正则化</li>
<li>Elastic-Net 回归源码解析：
<ul>
<li>alpha：混合惩罚项的常数</li>
<li>l1_ratio：弹性网混合参数，0 &lt;= l1_ratio &lt;= 1，对于 l1_ratio =
0，惩罚项是L2正则惩罚。对于 l1_ratio = 1是L1正则惩罚。对于 0</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>precompute：bool
类型，默认值为False，决定是否提前计算Gram矩阵来加速计算</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练</li>
</ul></li>
</ul>
<h3 id="多项式回归">4、多项式回归</h3>
<h4 id="多项式回归基本概念">4.1、多项式回归基本概念</h4>
<p>  升维的目的是为了去解决欠拟合的问题的，也就是为了提高模型的准确率为目的的，因为当维度不够时，说白了就是对于预测结果考虑的因素少的话，肯定不能准确的计算出模型。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/4c50d109e36a4027bceef52123ba5a50.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  在做升维的时候，最常见的手段就是将已知维度进行相乘（或者自乘）来构建新的维度，如下图所示。普通线性方程，无法拟合规律，必须是多项式，才可以完美拟合曲线规律，图中是二次多项式。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/0cb4bee00c86449892903be1bfdda1f0.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  对于多项式回归来说主要是为了扩展线性回归算法来适应更广泛的数据集，比如我们数据集有两个维度
<span class="math inline">\(x_1、x_2\)</span>，那么用多元线性回归公式就是：<span class="math inline">\(\hat{y} = w_0 + w_1x_1 +
w_2x_2\)</span>，当我们使用二阶多项式升维的时候，数据集就从原来的 <span class="math inline">\(x_1、x_2\)</span>扩展成了<span class="math inline">\(x_1、x_2、x_1^2、x_2^2、x_1x_2\)</span>
。因此多元线性回归就得去多计算三个维度所对应的w值：<span class="math inline">\(\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 +
w_4x_2^2 + w_5x_1x_2\)</span> 。</p>
<p>  此时拟合出来的方程就是曲线，可以解决一些线性回归的欠拟合问题！</p>
<h4 id="多项式回归实战1.0">4.2、多项式回归实战1.0</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，并进行可视化</span></span><br><span class="line">X = np.linspace(-<span class="number">1</span>,<span class="number">11</span>,num = <span class="number">100</span>)</span><br><span class="line">y = (X - <span class="number">5</span>)**<span class="number">2</span> + <span class="number">3</span>*X -<span class="number">12</span> + np.random.randn(<span class="number">100</span>)</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、创建预测数据</span></span><br><span class="line">X_test = np.linspace(-<span class="number">2</span>,<span class="number">12</span>,num = <span class="number">200</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、不进行升维 + 普通线性回归</span></span><br><span class="line">model_1 = LinearRegression()</span><br><span class="line">model_1.fit(X,y)</span><br><span class="line">y_test_1 = model_1.predict(X_test)</span><br><span class="line">plt.plot(X_test,y_test,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、多项式升维 + 普通线性回归</span></span><br><span class="line">X = np.concatenate([X,X**<span class="number">2</span>],axis = <span class="number">1</span>)</span><br><span class="line">model_2 = LinearRegression()</span><br><span class="line">model_2.fit(X,y)</span><br><span class="line"><span class="comment"># 5、测试数据处理，并预测</span></span><br><span class="line">X_test = np.concatenate([X_test,X_test**<span class="number">2</span>],axis = <span class="number">1</span>)</span><br><span class="line">y_test_2 = model_2.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、数据可视化，切片操作</span></span><br><span class="line">plt.plot(X_test[:,<span class="number">0</span>],y_test_2,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>不进行多项式升维，拟合出来的曲线，是线性的直线，和目标曲线无法匹配</li>
<li>使用np.concatenate()进行简单的，幂次合并，注意数据合并的方向axis =
1</li>
<li>数据可视化时，注意切片，因为数据升维后，多了平方这一维</li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/d4264371bc9041d997ec84b09d77e6fe.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="多项式回归实战2.0">4.3、多项式回归实战2.0</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures,StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，并进行可视化</span></span><br><span class="line">X = np.linspace(-<span class="number">1</span>,<span class="number">11</span>,num = <span class="number">100</span>)</span><br><span class="line">y = (X - <span class="number">5</span>)**<span class="number">2</span> + <span class="number">3</span>*X -<span class="number">12</span> + np.random.randn(<span class="number">100</span>)</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、创建预测数据</span></span><br><span class="line">X_test = np.linspace(-<span class="number">2</span>,<span class="number">12</span>,num = <span class="number">200</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、使用PolynomialFeatures进行特征升维</span></span><br><span class="line">poly = PolynomialFeatures()</span><br><span class="line">poly.fit(X,y)</span><br><span class="line">X = poly.transform(X)</span><br><span class="line">s = StandardScaler()</span><br><span class="line">X = s.fit_transform(X)</span><br><span class="line"><span class="comment"># model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.0001,max_iter = 10000)</span></span><br><span class="line">model = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,eta0 = <span class="number">0.01</span>)</span><br><span class="line">model.fit(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、预测数据</span></span><br><span class="line">X_test = poly.transform(X_test)</span><br><span class="line">X_test_norm = s.transform(X_test)</span><br><span class="line">y_test = model.predict(X_test_norm)</span><br><span class="line">plt.plot(X_test[:,<span class="number">1</span>],y_test,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>eta0表示学习率，设置合适的学习率，才能拟合成功</li>
<li>多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色</li>
<li>SGD随机梯度下降需要调整参数，以使模型适应数据</li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/8e8e751622bd4b10a6963673a8e3766b.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="代码实战天猫双十一销量预测">5、代码实战天猫双十一销量预测</h3>
<p>  天猫双十一，从2009年开始举办，第一届成交额仅仅0.5亿，后面呈现了爆发式的增长，那么这些增长是否有规律呢？是怎么样的规律，该如何分析呢？我们使用多项式回归一探究竟！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/31cb52d42edf43d6a46c382b4b40ce08.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>数据可视化，历年天猫双十一销量数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据，年份数据2009 ~ 2019</span></span><br><span class="line">X = np.arange(<span class="number">2009</span>,<span class="number">2020</span>)</span><br><span class="line">y = np.array([<span class="number">0.5</span>,<span class="number">9.36</span>,<span class="number">52</span>,<span class="number">191</span>,<span class="number">350</span>,<span class="number">571</span>,<span class="number">912</span>,<span class="number">1207</span>,<span class="number">1682</span>,<span class="number">2135</span>,<span class="number">2684</span>])</span><br><span class="line">plt.bar(X,y,width = <span class="number">0.5</span>,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.plot(X,y,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">_ = plt.xticks(ticks = X)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/10d4dd476d8e40daba21a97d77866da8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>有图可知，在一定时间内，随着经济的发展，天猫双十一销量与年份的关系是多项式关系！假定，销量和年份之间关系是三次幂关系：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/35f0479254104929b16b08ebb713fbc6.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">9</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，年份数据2009 ~ 2019</span></span><br><span class="line">X = np.arange(<span class="number">2009</span>,<span class="number">2020</span>)</span><br><span class="line">y = np.array([<span class="number">0.5</span>,<span class="number">9.36</span>,<span class="number">52</span>,<span class="number">191</span>,<span class="number">350</span>,<span class="number">571</span>,<span class="number">912</span>,<span class="number">1207</span>,<span class="number">1682</span>,<span class="number">2135</span>,<span class="number">2684</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、年份数据，均值移除，防止某一个特征列数据天然的数值太大而影响结果</span></span><br><span class="line">X = X - X.mean()</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、构建多项式特征，3次幂</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">X = poly.fit_transform(X)</span><br><span class="line">s = StandardScaler()</span><br><span class="line">X_norm = s.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建模型</span></span><br><span class="line">model = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,eta0 = <span class="number">0.5</span>,max_iter = <span class="number">5000</span>)</span><br><span class="line">model.fit(X_norm,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、数据预测</span></span><br><span class="line">X_test = np.linspace(-<span class="number">5</span>,<span class="number">6</span>,<span class="number">100</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">X_test = poly.transform(X_test)</span><br><span class="line">X_test_norm = s.transform(X_test)</span><br><span class="line">y_test = model.predict(X_test_norm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、数据可视化</span></span><br><span class="line">plt.plot(X_test[:,<span class="number">1</span>],y_test,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.bar(X[:,<span class="number">1</span>],y)</span><br><span class="line">plt.bar(<span class="number">6</span>,y_test[-<span class="number">1</span>],color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">4096</span>)</span><br><span class="line">plt.text(<span class="number">6</span>,y_test[-<span class="number">1</span>] + <span class="number">100</span>,<span class="built_in">round</span>(y_test[-<span class="number">1</span>],<span class="number">1</span>),ha = <span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">_ = plt.xticks(np.arange(-<span class="number">5</span>,<span class="number">7</span>),np.arange(<span class="number">2009</span>,<span class="number">2021</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/6a839d47304f4373b84be460374381a5.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>结论：</strong></p>
<ul>
<li>数据预处理，均值移除。如果特征<strong>基准值和分散度</strong>不同在某些算法（例如回归算法，KNN等）上可能会大大影响了模型的预测能力。通过均值移除，大大增强数据的<strong>离散化</strong>程度。</li>
<li>多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色</li>
<li>SGD随机梯度下降需要调整参数，以使模型适应多项式数据</li>
<li>从2020年开始，天猫双十一统计的成交额改变了规则为11.1日~11.11日的成交数据（之前的数据为双十一当天的数据），2020年成交额为<strong>4980</strong>亿元</li>
<li>可以，经济发展有其客观规律，前11年高速发展（曲线基本可以反应销售规律），到2020年是一个转折点</li>
</ul>
<h3 id="代码实战中国人寿保费预测">6、代码实战中国人寿保费预测</h3>
<h4 id="数据加载与介绍">6.1、数据加载与介绍</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;./中国人寿.xlsx&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>数据介绍：</p>
<ul>
<li>共计1338条保险数据，每条数据7个属性</li>
<li>最后一列charges是保费</li>
<li>前面6列是特征，分别为：年龄、性别、体重指数、小孩数量、是否抽烟、所在地区</li>
</ul>
<h4 id="eda数据探索">6.2、EDA数据探索</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment"># 性别对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;sex&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 地区对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;region&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 吸烟对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;smoker&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 孩子数量对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;children&#x27;</span>],palette=<span class="string">&#x27;Set1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/93b4a181835c4083a9e43c3383583369.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>总结：</p>
<ul>
<li>不同性别对保费影响不大，不同性别的保费的概率分布曲线基本重合，因此这个特征无足轻重，可以删除</li>
<li>地区同理</li>
<li>吸烟与否对保费的概率分布曲线差别很大，整体来说不吸烟更加健康，那么保费就低，这个特征很重要</li>
<li>家庭孩子数量对保费有一定影响</li>
</ul>
<h4 id="特征工程">6.3、特征工程</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">&#x27;region&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">data.head() <span class="comment"># 删除不重要特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 体重指数，离散化转换，体重两种情况：标准、肥胖</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">df,bmi</span>):</span><br><span class="line">    df[<span class="string">&#x27;bmi&#x27;</span>] = <span class="string">&#x27;fat&#x27;</span> <span class="keyword">if</span> df[<span class="string">&#x27;bmi&#x27;</span>] &gt;= bmi <span class="keyword">else</span> <span class="string">&#x27;standard&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line">data = data.apply(convert, axis = <span class="number">1</span>, args=(<span class="number">30</span>,))</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征提取，离散型数据转换为数值型数据</span></span><br><span class="line">data = pd.get_dummies(data)</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征和目标值抽取</span></span><br><span class="line">X = data.drop(<span class="string">&#x27;charges&#x27;</span>, axis=<span class="number">1</span>) <span class="comment"># 训练数据</span></span><br><span class="line">y = data[<span class="string">&#x27;charges&#x27;</span>] <span class="comment"># 目标值</span></span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure>
<h4 id="特征升维">6.4、特征升维</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_squared_log_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据拆分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征升维</span></span><br><span class="line">poly = PolynomialFeatures(degree= <span class="number">2</span>, include_bias = <span class="literal">False</span>)</span><br><span class="line">X_train_poly = poly.fit_transform(X_train)</span><br><span class="line">X_test_poly = poly.fit_transform(X_test)</span><br></pre></td></tr></table></figure>
<h4 id="模型训练与评估">6.5、模型训练与评估</h4>
<p>普通线性回归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_1 = LinearRegression()</span><br><span class="line">model_1.fit(X_train_poly, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据得分：&#x27;</span>,model_1.score(X_train_poly,y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测数据得分：&#x27;</span>,model_1.score(X_test_poly,y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据均方误差：&#x27;</span>,np.sqrt(mean_squared_error(y_train,model_1.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据均方误差：&#x27;</span>,np.sqrt(mean_squared_error(y_test,model_1.predict(X_test_poly))))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据对数误差：&#x27;</span>,np.sqrt(mean_squared_log_error(y_train,model_1.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据对数误差：&#x27;</span>,np.sqrt(mean_squared_log_error(y_test,model_1.predict(X_test_poly))))</span><br></pre></td></tr></table></figure>
<p>弹性网络回归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model_2 = ElasticNet(alpha = <span class="number">0.3</span>,l1_ratio = <span class="number">0.5</span>,max_iter = <span class="number">50000</span>)</span><br><span class="line">model_2.fit(X_train_poly,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据得分：&#x27;</span>,model_2.score(X_train_poly,y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测数据得分：&#x27;</span>,model_2.score(X_test_poly,y_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据均方误差为：&#x27;</span>,np.sqrt(mean_squared_error(y_train,model_2.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据均方误差为：&#x27;</span>,np.sqrt(mean_squared_error(y_test,model_2.predict(X_test_poly))))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据对数误差为：&#x27;</span>,np.sqrt(mean_squared_log_error(y_train,model_2.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据对数误差为：&#x27;</span>,np.sqrt(mean_squared_log_error(y_test,model_2.predict(X_test_poly))))</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>进行EDA数据探索，可以查看无关紧要特征</li>
<li>进行特征工程：删除无用特征、特征离散化、特征提取。这对机器学习都至关重要</li>
<li>对于简单的数据（特征比较少）进行线性回归，一般需要进行特征升维</li>
<li>选择不同的算法，进行训练和评估，从中筛选优秀算法</li>
</ul>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="../../img/avatar.png" alt="albert"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">albert</p><p class="is-size-6 is-block">albert</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>shanghai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="../../archives"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="../../categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="../../tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/0914ds" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="../../index.html"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="../JVM%E8%B0%83%E4%BC%98%E5%90%88%E9%9B%86/"><span class="level-start"><span class="level-item">JVM调优合集</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="../%E5%AE%B9%E5%99%A8/"><span class="level-start"><span class="level-item">容器</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="../%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">数据库</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href=""><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"><span class="level-start"><span class="level-item">消息中间件</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-17T17:13:16.000Z">2023-08-18</time></p><p class="title"><a href="../../2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">多元线性回归</a></p><p class="categories"><a href="">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-17T05:13:16.000Z">2023-08-17</time></p><p class="title"><a href="../../2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">多元线性回归</a></p><p class="categories"><a href="">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-16T17:30:16.000Z">2023-08-17</time></p><p class="title"><a href="../../2023/08/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96/">梯度下降优化</a></p><p class="categories"><a href="">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-17T06:13:16.000Z">2023-06-17</time></p><p class="title"><a href="../../2023/06/17/%E6%B6%88%E6%81%AF/kafka/">kafka</a></p><p class="categories"><a href="../%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/">消息中间件</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-05T07:02:29.235Z">2023-06-05</time></p><p class="title"><a href="../../2023/06/05/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/">mysql的锁机制</a></p><p class="categories"><a href="../%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="../../archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="../../archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="../../index.html"><img src="../../img/logo.svg" alt="blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 albert dong</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="../../js/column.js"></script><script src="../../js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="../../js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="../../js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="../../js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"../../content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>