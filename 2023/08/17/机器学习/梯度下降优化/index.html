<!DOCTYPE html>
<html lang="en">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="梯度下降优化" />
    <meta name="hexo-theme-A4" content="v1.9.7" />
    <link rel="alternate icon" type="image/webp" href="../../../../../img/favicon.webp">
    <title>blog</title>

    
        
<link rel="stylesheet" href="../../../../../css/highlight/style1.css">

        
<link rel="stylesheet" href="../../../../../css/reset.css">

        
<link rel="stylesheet" href="../../../../../css/markdown.css">

        
<link rel="stylesheet" href="../../../../../css/fonts.css">
 
         <!--注意：首页既不是post也不是page-->
        
        
        
<link rel="stylesheet" href="../../../../../css/ui.css">
 
        
<link rel="stylesheet" href="../../../../../css/style.css">


        
            <!--返回顶部css-->
            
<link rel="stylesheet" href="../../../../../css/returnToTop.css">

            
<link rel="stylesheet" href="../../../../../css/unicons.css">

        
        
            <!--目录-->
            
<link rel="stylesheet" href="../../../../../css/toc.css">

        
    

    
        
<link rel="stylesheet" href="../../../../../css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="../../../../../css/lightgallery-bundle.min.css">


   
        
<link rel="stylesheet" href="../../../../../css/custom.css">

    

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="atom.xml" title="blog" type="application/atom+xml">
</head>
    
    
        <style>
            .index-main{
                max-width:  880px;
            }
        </style>

    
    



    

    
    




    
    
    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #e4e4e4 ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: '🌓', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            <div class="left-toc-container">
                <nav id="toc" class="bs-docs-sidebar"></nav>
            </div>
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <style>
            .header-img {
                width: 56px;
                height: auto;
                object-fit: cover; /* 保持图片比例 */
                transition: transform 0.3s ease-in-out; 
                border-radius: 0; 
            }
            
        </style>
        <img 
            alt="^-^" 
            cache-control="max-age=86400" 
            class="header-img" 
            src="../../../../../img/favicon.webp" 
        />
        <div class="header-content">
            <a class="logo" href="../../../../../index.html">blog</a> 
            <span class="description"></span> 
        </div>
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="../../../../../index.html">首页</a></li>
            
        
            
                <li><a href="../../../../../list/">文章</a></li>
            
        
            
                <li><a href="../../../../../about/">关于</a></li>
            
        
            
                <li><a href="../../../../../tags/">标签</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--说明是文章post页面-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    梯度下降优化
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                    <span>最近更新：2023-08-18</span> 
                
                
                    
                        &nbsp; | &nbsp;
                    
                     <span>字数总计：8.6k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>阅读估时：34分钟</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span id="busuanzi_container_page_pv">
                        阅读量：<span id="busuanzi_value_page_pv"></span>次
                    </span>
                
            </div>
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96"><span class="post-toc-text">梯度下降优化</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96-normalization"><span class="post-toc-text">1、归一化 Normalization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E7%9B%AE%E7%9A%84"><span class="post-toc-text">1.1、归一化目的</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%BD%92%E4%B8%80%E5%8C%96%E6%9C%AC%E8%B4%A8"><span class="post-toc-text">1.2、归一化本质</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%9C%80%E5%A4%A7%E5%80%BC%E6%9C%80%E5%B0%8F%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96"><span class="post-toc-text">1.3、最大值最小值归一化</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%9D%87%E5%80%BC%E6%A0%87%E5%87%86%E5%8C%96"><span class="post-toc-text">1.4、0-均值标准化</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96-regularization"><span class="post-toc-text">2、正则化 Regularization</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="post-toc-text">2.1、过拟合欠拟合</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%A5%97%E7%B4%A2%E5%9B%9E%E5%BD%92lasso"><span class="post-toc-text">2.2、套索回归（Lasso）</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92ridge"><span class="post-toc-text">2.3、岭回归（Ridge）</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E8%A1%8D%E7%94%9F%E7%AE%97%E6%B3%95"><span class="post-toc-text">3、线性回归衍生算法</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#ridge%E7%AE%97%E6%B3%95%E4%BD%BF%E7%94%A8"><span class="post-toc-text">3.1、Ridge算法使用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#lasso%E7%AE%97%E6%B3%95%E4%BD%BF%E7%94%A8"><span class="post-toc-text">3.2、Lasso算法使用</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#elastic-net%E7%AE%97%E6%B3%95%E4%BD%BF%E7%94%A8"><span class="post-toc-text">3.3、Elastic-Net算法使用</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="post-toc-text">4、多项式回归</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="post-toc-text">4.1、多项式回归基本概念</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%88%981.0"><span class="post-toc-text">4.2、多项式回归实战1.0</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%E5%AE%9E%E6%88%982.0"><span class="post-toc-text">4.3、多项式回归实战2.0</span></a></li></ol></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98%E5%A4%A9%E7%8C%AB%E5%8F%8C%E5%8D%81%E4%B8%80%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B"><span class="post-toc-text">5、代码实战天猫双十一销量预测</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E6%88%98%E4%B8%AD%E5%9B%BD%E4%BA%BA%E5%AF%BF%E4%BF%9D%E8%B4%B9%E9%A2%84%E6%B5%8B"><span class="post-toc-text">6、代码实战中国人寿保费预测</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E4%BB%8B%E7%BB%8D"><span class="post-toc-text">6.1、数据加载与介绍</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#eda%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2"><span class="post-toc-text">6.2、EDA数据探索</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="post-toc-text">6.3、特征工程</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E7%89%B9%E5%BE%81%E5%8D%87%E7%BB%B4"><span class="post-toc-text">6.4、特征升维</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E8%AF%84%E4%BC%B0"><span class="post-toc-text">6.5、模型训练与评估</span></a></li></ol></li></ol></li></ol>
            
        
        <div class=".article-gallery"><meta name="referrer" content="no-referrer">
<h2 id="梯度下降优化">梯度下降优化</h2>
<h3 id="归一化-normalization">1、归一化 Normalization</h3>
<h4 id="归一化目的">1.1、归一化目的</h4>
<p>  梯度下降的原理和应用，我们已经在前面课程中进行了学习，大家仔细观察下图。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/c361fdfa8946471c97c40e111bc4c17d.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  不同方向的<strong>陡峭度</strong>是不一样的，即不同维度的数值大小是不同。也就是说梯度下降的快慢是不同的：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/83a1603692cc44d4ab90672dc31b2fde.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p> 如果维度多了，就是<strong>超平面</strong>（了解一下霍金所说的宇宙十一维空间），很难画出来了，感受一下下面这张图的空间维度情况。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/60843367a65a49dcb8f61bd689aa7e68.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  如果拿多元线性回归举例的话，因为多元线性回归的损失函数 MSE
是凸函数，所以我们可以把损失函数看成是一个碗。然后下面的图就是从碗上方去俯瞰！哪里是损失最小的地方呢？当然对应的就是碗底的地方！所以下图碗中心的地方颜色较浅的区域就是损失函数最小的地方。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/8577307ca13a44b29a344ed49bde7704.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  上面两张图都是进行梯度下降，你有没有发现，略有不同啊？两张图形都是鸟瞰图，左边的图形做了归一化处理，右边是没有做归一化的俯瞰图。</p>
<p>  啥是归一化呢？请带着疑问跟我走~</p>
<p>  我们先来说一下为什么没做归一化是右侧图示，举个例子假如我们客户数据信息，有两个维度，一个是用户的年龄，一个是用户的月收入，目标变量是快乐程度。</p>
<table>
<thead>
<tr>
<th style="text-align: center;">name</th>
<th style="text-align: center;">age</th>
<th style="text-align: center;">salary</th>
<th style="text-align: center;">happy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">路博通</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">7000</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">马老师</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">20000</td>
<td style="text-align: center;">180</td>
</tr>
<tr>
<td style="text-align: center;">赵老师</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">30000</td>
<td style="text-align: center;">164</td>
</tr>
<tr>
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
<td style="text-align: center;">……</td>
</tr>
</tbody>
</table>
<p>  我们可以里面写出线性回归公式， <span class="math inline">\(y =
\theta_1x_1 + \theta_2x_2 + b\)</span>
，那么这样每一条样本不同维度对应的数量级不同，原因是每个维度对应的物理含义不同嘛，但是计算机能理解
36 和 7000 分别是年龄和收入吗？计算机只是拿到一堆数字而已。</p>
<p>  我们把 <span class="math inline">\(x_1\)</span> 看成是年龄，<span class="math inline">\(x_2\)</span> 看成是收入， y
对应着快乐程度。机器学习就是在知道
X，y的情况下解方程组调整出最优解的过程。根据公式我们也可以发现 y
是两部分贡献之和，按常理来说，一开始并不知道两个部分谁更重要的情况下，可以想象为两部分对
y 的贡献是一样的即 <span class="math inline">\(\theta_1x_1 =
\theta_2x_2\)</span> ，如果 <span class="math inline">\(x_1 \ll
x_2\)</span> ，那么最终 <span class="math inline">\(\theta_1 \gg
\theta_2\)</span> （远大于）。</p>
<p>  这样是不是就比较好理解为什么之前右侧示图里为什么 <span class="math inline">\(\theta_1 &gt; \theta_2\)</span>
，看起来就是椭圆。再思考一下，梯度下降第 1 步的操作，是不是所有的维度
<span class="math inline">\(\theta\)</span> 都是根据在期望 <span class="math inline">\(\mu\)</span> 为 0 方差 <span class="math inline">\(\sigma\)</span> 为 1
的正太分布随机生成的，说白了就是一开始的 <span class="math inline">\(\theta_1\)</span> 和 <span class="math inline">\(\theta_2\)</span> 数值是差不多的。所以可以发现
<span class="math inline">\(\theta_1\)</span> 从初始值到目标位置 <span class="math inline">\(\theta_1^{target}\)</span> 的距离要远大于 <span class="math inline">\(\theta_2\)</span> 从初始值到目标位置<span class="math inline">\(\theta_2^{target}\)</span>。</p>
<p>  因为 <span class="math inline">\(x_1 \ll x_2\)</span>，根据梯度公式
<span class="math inline">\(g_j= (h_{\theta}(x) - y)x_j\)</span> ，得出
<span class="math inline">\(g_1 \ll
g_2\)</span>。根据梯度下降公式：<span class="math inline">\(\theta_j^{n+1} = \theta_j^n - \eta * g_j\)</span>
可知，每次调整 <span class="math inline">\(\theta_1\)</span> 的幅度
<span class="math inline">\(\ll\)</span> （远小于） <span class="math inline">\(\theta_2\)</span> 的调整幅度。</p>
<p>  总结一下 ，根据上面得到的两个结论 ，它俩之间是互相矛盾的
，意味着最后 <span class="math inline">\(\theta_2\)</span> 需要比 <span class="math inline">\(\theta_1\)</span>
更少的迭代次数就可以收敛，而我们要最终求得最优解，就必须每个维度 <span class="math inline">\(\theta\)</span> 都收敛才可以，所以会出现 <span class="math inline">\(\theta_2\)</span> 等待 <span class="math inline">\(\theta_1\)</span>
收敛的情况。讲到这里对应图大家应该可以理解为什么右图是先顺着 <span class="math inline">\(\theta_2\)</span>
的坐标轴往下走再往右走的原因了吧。</p>
<p><strong>结论:</strong></p>
<p>  归一化的一个目的是，使得梯度下降在不同维度 <span class="math inline">\(\theta\)</span>
参数（不同数量级）上，可以步调一致协同的进行梯度下降。这就好比社会主义，一小部分人先富裕起来了，先富带后富，这需要一定的时间，先富的这批人等待其他的人富裕起来；但是，更好途经是实现共同富裕，最后每个人都不能落下，
优化的步伐是一致的。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/46723e3f15054b108136b59fa0836252.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>经过归一化处理，收敛的速度，明显快了！</p>
<h4 id="归一化本质">1.2、归一化本质</h4>
<p>  做归一化的目的是要实现<strong>共同富裕</strong>，而之所以梯度下降优化时不能达到步调一致的根本原因其实还是
<span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 的数量级不同。所以什么是归一化？</p>
<p>  答案自然就出来了，就是把 <span class="math inline">\(x_1\)</span>
和 <span class="math inline">\(x_2\)</span>
的数量级统一，扩展一点说，如果有更多特征维度，就要把各个特征维度 <span class="math inline">\(x_1、x_2、……、x_n\)</span>
的数量级统一，来做到无量纲化。</p>
<h4 id="最大值最小值归一化">1.3、最大值最小值归一化</h4>
<p>  也称为离差标准化，是对原始数据的线性变换，<strong>使结果值映射到[0
- 1]之间</strong>。转换函数如下：</p>
<p><span class="math inline">\(X^* = \frac{X - X\_min}{X\_max
-X\_min}\)</span></p>
<p>  其实我们很容易发现使用最大值最小值归一化（min-max标准化）的时候，优点是一定可以把数值归一到
0 ~ 1
之间，缺点是如果有一个离群值（比如马云的财富），正如我们举的例子一样，会使得一个数值为
1，其它数值都几乎为 0，所以受离群值的影响比较大！</p>
<p><strong>代码演示：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">x_ = (x - x.<span class="built_in">min</span>(axis = <span class="number">0</span>)) / (x.<span class="built_in">max</span>(axis = <span class="number">0</span>) - x.<span class="built_in">min</span>(axis = <span class="number">0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p><strong>使用scikit-learn函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">min_max_scaler = MinMaxScaler()</span><br><span class="line">x_ = min_max_scaler.fit_transform(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<h4 id="均值标准化">1.4、0-均值标准化</h4>
<p>  这种方法给予原始数据的均值（mean）和标准差（standard
deviation）进行数据的标准化，也叫做Z-score标准化。经过处理的数据符合标准正态分布，即均值为0，标准差为1，转化函数为：</p>
<p><span class="math inline">\(X^* = \frac{X - \mu}{\sigma}\)</span></p>
<p>其中μ为所有样本数据的均值，σ为所有样本数据的标准差。</p>
<p><span class="math inline">\(\mu = \frac{1}{n}\sum\limits_{i =
1}^nx_i\)</span></p>
<p><span class="math inline">\(\sigma = \sqrt{\frac{1}{n}\sum\limits_{i
= 1}^n(x_i - \mu)^2}\)</span></p>
<p>  相对于最大值最小值归一化来说，因为标准归一化除以了标准差，而标准差的计算会考虑到所有样本数据，所以受到离群值的影响会小一些，这就是除以方差的好处！但是，0-均值标准化不一定会把数据缩放到
0 ~ 1 之间了。既然是0均值，也就意味着，有正有负！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">x_ = (x - x.mean(axis = <span class="number">0</span>)) / x.std(axis = <span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p><strong>使用scikit-learn函数：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">x_1 = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">10</span>)</span><br><span class="line">x_2 = np.random.randint(<span class="number">100</span>,<span class="number">300</span>,size = <span class="number">10</span>)</span><br><span class="line">x = np.c_[x_1,x_2]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之前的数据：&#x27;</span>)</span><br><span class="line">display(x)</span><br><span class="line">standard_scaler = StandardScaler()</span><br><span class="line">x_ = standard_scaler.fit_transform(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;归一化之后的数据：&#x27;</span>)</span><br><span class="line">display(x_)</span><br></pre></td></tr></table></figure>
<p>  那为什么要减去均值呢？其实做均值归一化还有一个特殊的好处（对比最大值最小值归一化，全部是正数0~1），我们来看一下梯度下降的式子，你就会发现
<span class="math inline">\(\alpha\)</span> 是正数，不管 A 是正还是负（
A 就是 <span class="math inline">\(\hat{y} - y = h_{\theta}(x) -
y\)</span>），对于所有的维度 X，比如这里的 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 来说，<span class="math inline">\(\alpha\)</span> 乘上 A
都是一样的符号，那么每次迭代的时候 <span class="math inline">\(w_1^{t+1}\)</span> 和 <span class="math inline">\(w_2^{t+1}\)</span>
的更新幅度符号也必然是一样的，这样就会像下图有右侧所示：要想从 <span class="math inline">\(w_t\)</span> 更新到 <span class="math inline">\(w^*\)</span> 就必然要么 <span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span> 同时变大再同时变小，或者就 <span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span>
同时变小再同时变大。不能如图上所示蓝色的最优解路径，即 <span class="math inline">\(w_1\)</span> 变小的同时 <span class="math inline">\(w_2\)</span> 变大！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/2cdaf2f432bb4e419ed2e690197df8b6.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  那我们如何才能做到让 <span class="math inline">\(w_1\)</span>
变小的时候 <span class="math inline">\(w_2\)</span>
变大呢？归其根本还是数据集 X
矩阵（经过min-max归一化）中的数据均为正数。所以如果我们可以让 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span>
它们符号不同，比如有正有负，其实就可以在做梯度下降的时候有更多的可能性去让更新尽可能沿着最优解路径去走。</p>
<p>  结论：<strong>0-均值标准化</strong>处理数据之后，属性有正有负，可以让梯度下降沿着最优路径进行~</p>
<p><strong>注意：</strong></p>
<p>  我们在做特征工程的时候，很多时候如果对训练集的数据进行了预处理，比如这里讲的归一化，那么未来对测试集的时候，和模型上线来新的数据的时候，都要进行<strong>相同的</strong>数据预处理流程，而且所使用的均值和方差是来自当时训练集的均值和方差!</p>
<p>  因为我们人工智能要干的事情就是从训练集数据中找规律，然后利用找到的规律去预测新产生的数据。这也就是说假设训练集和测试集以及未来新来的数据是属于同分布的！从代码上面来说如何去使用训练集的均值和方差呢？就需要把
scaler 对象持久化，
回头模型上线的时候再加载进来去对新来的数据进行处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line">joblib.dump(standard_scaler,<span class="string">&#x27;scale&#x27;</span>) <span class="comment"># 持久化</span></span><br><span class="line">standard_scaler = joblib.load(<span class="string">&#x27;scale&#x27;</span>) <span class="comment"># 加载</span></span><br><span class="line">standard_scaler.transform(x) <span class="comment"># 使用</span></span><br></pre></td></tr></table></figure>
<h3 id="正则化-regularization">2、正则化 Regularization</h3>
<h4 id="过拟合欠拟合">2.1、过拟合欠拟合</h4>
<ol type="1">
<li>欠拟合（under
fit）：还没有拟合到位，训练集和测试集的准确率都还没有到达最高，学的还不到位。</li>
<li>过拟合（over
fit）：拟合过度，训练集的准确率升高的同时，测试集的准确率反而降低。学的过度了（走火入魔），做过的卷子都能再次答对（死记硬背），考试碰到新的没见过的题就考不好（不会举一反三）。</li>
<li>恰到好处（just
right）：过拟合前，训练集和测试集准确率都达到巅峰。好比，学习并不需要花费很多时间，理解的很好，考试的时候可以很好的把知识举一反三。</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/793562d28f114fa5924bbcd3f6361786.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  正则化就是防止过拟合，增加模型的<strong>鲁棒性</strong>，鲁棒是
Robust
的音译，也就是强壮的意思。就像计算机软件在面临攻击、网络过载等情况下能够不死机不崩溃，这就是软件的鲁棒性。鲁棒性调优就是让模型拥有更好的鲁棒性，也就是让模型的泛化能力和推广
能力更加的强大。</p>
<p>  举例子说明：下面两个式子描述同一条直线那个更好？</p>
<p><span class="math inline">\(y = 0.3x_1 + 0.4x_2 + 0.5\)</span></p>
<p><span class="math inline">\(y = 3x_1 + 4x_2 + 5\)</span></p>
<p>  第一个更好，因为下面的公式是上面的十倍，当 w
越小公式的容错的能力就越好。因为把测试数据带入公式中如果测试集原来是
[32, 128] 在带入的时候发生了一些偏差，比如说变成 [30, 120]
，第二个模型结果就会比第一个模型结果的偏差大的多。公式中 <span class="math inline">\(y = W^Tx\)</span> ，当 x
有一点错误，这个错误会通过 w 放大。但是 w 不能太小，当 w
太小时（比如都趋近0），模型就没有意义了，无法应用。想要有一定的容错率又要保证正确率就要由正则项来发挥作用了！</p>
<p>  所以正则化(鲁棒性调优)的本质就是牺牲模型在训练集上的正确率来提高推广、泛化能力，
W
在数值上越小越好，这样能抵抗数值的<strong>扰动</strong>。同时为了保证模型的正确率
W 又不能极小。
故而人们将原来的损失函数加上一个惩罚项，这里面损失函数就是原来固有的损失函数，比如回归的话通常是
MSE，分类的话通常是 cross entropy
交叉熵，然后在加上一部分惩罚项来使得计算出来的模型 W
相对小一些来带来泛化能力。</p>
<p>  常用的惩罚项有L1 正则项或者 L2 正则项：</p>
<ul>
<li><span class="math inline">\(L_1 = ||w||_1 = \sum\limits_{i =
1}^n|w_i|\)</span> 对应曼哈顿距离</li>
<li><span class="math inline">\(L_2 = ||w||_2 = \sqrt{\sum\limits_{i =
1}^n(w_i)^2}\)</span> 对应欧氏距离</li>
</ul>
<p>其实 L1 和 L2
正则的公式数学里面的意义就是范数，代表空间中向量到原点的距离：</p>
<p><span class="math inline">\(L_p = ||X||_p = \sqrt[p]{\sum\limits_{i =
1}^nx_i^p} , X = (x_1,x_2,……x_n)\)</span></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/95727c537b1e41a39c1a698bae619d7d.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  当我们把多元线性回归损失函数加上 L2 正则的时候，就诞生了 Ridge
岭回归。当我们把多元线性回归损失函数加上 L1 正则的时候，就孕育出来了
Lasso 回归。其实 L1 和 L2
正则项惩罚项可以加到任何算法的损失函数上面去提高计算出来模型的泛化能力的。</p>
<h4 id="套索回归lasso">2.2、套索回归（Lasso）</h4>
<p>先从线性回归开始，其损失函数如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/3c449aa80d4d4b5cb02114ea1d347445.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L1正则化的损失函数，令<span class="math inline">\(J_0 =
J(\theta)\)</span>：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/bdb461ed1a6746258baf6fe48988627c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>令 <span class="math inline">\(L_1 = \alpha * \sum\limits_{i =
1}^n|w_i|\)</span> ：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ac47115c4dcb437f8c96c799530a142d.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  其中 <span class="math inline">\(J_0\)</span>
是原始的损失函数，加号后面的一项是L1正则化项， <span class="math inline">\(\alpha\)</span> 是正则化系数。注意到
L1正则化是权值的绝对值之和。<span class="math inline">\(J\)</span>
是带有绝对值符号的函数，因此 <span class="math inline">\(J\)</span>
是不完全可微的。机器学习的任务就是要通过一些方法（比如梯度下降）求出损失函数的最小值。当我们在原始损失函数
<span class="math inline">\(J_0\)</span> 后面添加L1正则项时，相当于对
<span class="math inline">\(J_0\)</span> 做了一个约束。令<span class="math inline">\(L_1 = \alpha * \sum\limits_{i = 1}^n|w_i|\)</span>
，则 <span class="math inline">\(J = J_0 + L_1\)</span>
，此时我们的任务变成在 <span class="math inline">\(L_1\)</span>
约束下求出 <span class="math inline">\(J_0\)</span>
取最小值的解。<strong>考虑二维的情况</strong>，即只有两个权值 <span class="math inline">\(w_1、w_2\)</span> ，此时 <span class="math inline">\(L_1 = |w_1| + |w_2|\)</span>。
对于梯度下降法，求解 <span class="math inline">\(J_0\)</span>
过程可以画出等值线，同时 L1 正则化的函数 <span class="math inline">\(L_1\)</span> 也可以在 <span class="math inline">\(w_1、w_2\)</span>所在的平面上画出来：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ebbe4decd1e7445c9df9085f85b26049.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  图中等值线是<span class="math inline">\(J_0\)</span>的等值线，是椭圆形。黑色方框是 <span class="math inline">\(L_1\)</span> 函数的图形，<span class="math inline">\(L_1 = |w_1| + |w_2|\)</span>
这个函数画出来，就是一个方框。</p>
<p>  在图中，当 <span class="math inline">\(J_0\)</span> 等值线与 <span class="math inline">\(L_1\)</span> 图形首次相交的地方就是最优解。上图中
<span class="math inline">\(J_0\)</span> 与 <span class="math inline">\(L_1\)</span> 在 <span class="math inline">\(L_1\)</span>
的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是 <span class="math inline">\((w_1,w_2) = (0,w)\)</span> 。可以直观想象，因为
<span class="math inline">\(L_1\)</span>
函数有很多『突出的角』（二维情况下四个，多维情况下更多）， <span class="math inline">\(J_0\)</span> 与这些角接触的机率会远大于与 <span class="math inline">\(L_1\)</span>
其它部位接触的机率（这是很直觉的想象，突出的角比直线的边离等值线更近写），而在这些角上，会有很多权值等于0（因为角就在坐标轴上），这就是为什么
L1 正则化可以产生稀疏模型（很多权重等于0了），进而可以用于特征选择。</p>
<p>  而正则化前面的系数 <span class="math inline">\(\alpha\)</span>，可以控制 <span class="math inline">\(L_1\)</span> 图形的大小。<span class="math inline">\(\alpha\)</span> 越小，<span class="math inline">\(L_1\)</span> 的图形越大（上图中的黑色方框）；<span class="math inline">\(\alpha\)</span> 越大，<span class="math inline">\(L_1\)</span>
的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优解的值<span class="math inline">\((w_1,w_2) = (0,w)\)</span> 中的 w
可以取到很小的值的原因所在。</p>
<p>代码演示 <span class="math inline">\(\alpha\)</span>
取值大小对黑色方框的尺寸影响：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># α 的值是：1</span></span><br><span class="line"><span class="comment"># 1 = x + y</span></span><br><span class="line"><span class="comment"># y = 1 -x</span></span><br><span class="line">f = <span class="keyword">lambda</span> x : <span class="number">1</span>- x</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">100</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">plt.plot(x, f(x), color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># α 的值是：3</span></span><br><span class="line"><span class="comment"># 1 = 3 * x + 3 * y</span></span><br><span class="line"><span class="comment"># y = 1/3 -x</span></span><br><span class="line">f2 = <span class="keyword">lambda</span> x : <span class="number">1</span>/<span class="number">3</span> - x </span><br><span class="line">x2 = np.linspace(<span class="number">0</span>,<span class="number">1</span>/<span class="number">3</span>,<span class="number">100</span>)</span><br><span class="line">plt.plot(x2, f2(x2),color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一些列设置</span></span><br><span class="line">plt.xlim(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.ylim(-<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.spines[<span class="string">&#x27;right&#x27;</span>].set_color(<span class="string">&#x27;None&#x27;</span>)  <span class="comment"># 将图片的右框隐藏</span></span><br><span class="line">ax.spines[<span class="string">&#x27;top&#x27;</span>].set_color(<span class="string">&#x27;None&#x27;</span>)  <span class="comment"># 将图片的上边框隐藏</span></span><br><span class="line">ax.spines[<span class="string">&#x27;bottom&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>, <span class="number">0</span>)) <span class="comment"># x轴出现在y轴的-1 位置</span></span><br><span class="line">ax.spines[<span class="string">&#x27;left&#x27;</span>].set_position((<span class="string">&#x27;data&#x27;</span>, <span class="number">0</span>))</span><br><span class="line">plt.savefig(<span class="string">&#x27;./图片/13-alpha对方框影响.png&#x27;</span>,dpi = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/36f894d8be794c7fb0cfd6655acc1e85.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>权重更新规则如下：</strong></p>
<ol type="1">
<li>损失函数：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/873b8438cc4d46c39e88308f6fe63d6f.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="2" type="1">
<li>更新规则：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/deb751cc8eab47b0b620e2f8edae185c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(J_0\)</span>
即是线性回归的损失函数，<span class="math inline">\(L_1\)</span>
是添加的正则项。<span class="math inline">\(sgn(w_i)\)</span>
表示符号函数、指示函数，值为：1 或 -1。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/18bf2efda1204dd08081f1cdc9198a0a.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>注意当 <span class="math inline">\(w_i = 0\)</span> 时不可导。</p>
<p><strong>综上所述</strong>，L1正则化权重更新如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/05cfc081b4374d4aa32a3e76e5426f01.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ul>
<li>Lasso回归和线性回归相比，多了一项：<span class="math inline">\(-\eta
* \alpha * sgn(w_i)\)</span></li>
<li>$$ 大于零，表示梯度下降学习率</li>
<li><span class="math inline">\(\alpha\)</span>
大于零，表示L1正则化系数</li>
<li>当<span class="math inline">\(w_i\)</span>为正时候 <span class="math inline">\(sgn(w_i) = 1\)</span>，直接减去 <span class="math inline">\(\eta * \alpha\)</span> （大于0），所以正的 <span class="math inline">\(w_i\)</span> 变小了</li>
<li>当<span class="math inline">\(w_i\)</span>为负时候 <span class="math inline">\(sgn(w_i) = -1\)</span>，相当于直接加上 <span class="math inline">\(\eta * \alpha\)</span> （大于0），所以负的 <span class="math inline">\(w_i\)</span> 变大了，绝对值变小，向0靠近</li>
</ul>
<p>有的书本上公式会这样写，其中 <span class="math inline">\(\lambda\)</span> 表示L1正则化系数：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/f271019eeee445a48346efbda1acca44.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="岭回归ridge">2.3、岭回归（Ridge）</h4>
<p>也是先从线性回归开始，其损失函数如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/1c5b0185a068410eb3f3c1a3f82f0c72.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L2正则化的损失函数（对L2范数，进行了平方运算），令<span class="math inline">\(J_0 = J(\theta)\)</span>：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/b844b823dfcc4437ae7afd0f1f0b680c.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>令 <span class="math inline">\(L_2 = \alpha * \sum\limits_{i =
1}^n(w_i)^2\)</span> ：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/1cbddf3ce5da4e989c860fb392e93655.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>同样可以画出他们在二维平面上的图形，如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/ed5a6039416f48c1932f6565919d67f7.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>二维平面下 L2
正则化的函数图形是个圆（绝对值的平方和，是个圆），与方形相比，被磨去了棱角。因此
<span class="math inline">\(J_0\)</span> 与 <span class="math inline">\(L_2\)</span> 相交时使得 <span class="math inline">\(w_1、w_2\)</span>
等于零的机率小了许多（这个也是一个很直观的想象），这就是为什么L2正则化不具有稀疏性的原因，因为不太可能出现多数
w 都为0的情况（这种情况就叫稀疏性）！</p>
<p><strong>权重更新规则如下：</strong></p>
<ol type="1">
<li>损失函数：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/d13ef5f48f584dc28a629673647035a2.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<ol start="2" type="1">
<li>更新规则：</li>
</ol>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/3bc2481fb852443a98553e4e95ad670b.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(J_0\)</span>
即是线性回归的损失函数，<span class="math inline">\(L_2\)</span>
是添加的正则项。</p>
<p><strong>综上所述</strong>，L2正则化权重更新如下（$ 2$
也是常数项，可以合并到一起用整体 <span class="math inline">\(\alpha\)</span> 替代）：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/54a6992795ec49e8af1f268e3ab0f283.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 <span class="math inline">\(\alpha\)</span>
就是正则化参数，<span class="math inline">\(\eta\)</span>
表示学习率。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，
<span class="math inline">\(\theta_j\)</span>
都要先乘以一个小于1的因子（即 <span class="math inline">\((1-\eta *
\alpha)\)</span> ），从而使得 <span class="math inline">\(\theta_j\)</span> 加速减小，因此总的来看，<span class="math inline">\(\theta\)</span>
相比不加L2正则项的线性回归可以获得更小的值。从而，实现了防止过拟合的效果，增加模型的鲁棒性~</p>
<p>有的书本上，公式写法可能<strong>不同</strong>：其中 <span class="math inline">\(\lambda\)</span> 表示正则化参数。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/0910c26576bd4e159e46c0770caa1995.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="线性回归衍生算法">3、线性回归衍生算法</h3>
<p>  接下来，我们一起学习一下scikit-learn中为我们提供的线性回归衍生算法，根据上面所学的原理，对比线性回归加深理解。</p>
<h4 id="ridge算法使用">3.1、Ridge算法使用</h4>
<p>这是scikit-learn官网给出的岭回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/c16bea0c68cd4e749269f2b4d089d73f.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>L2正则化和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">ridge = Ridge(alpha= <span class="number">1</span>, solver=<span class="string">&#x27;sag&#x27;</span>)</span><br><span class="line">ridge.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;岭回归求解的斜率：&#x27;</span>,ridge.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;岭回归求解的截距：&#x27;</span>,ridge.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>,l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知L2正则化，将方程系数进行了缩小</li>
<li><span class="math inline">\(\alpha\)</span>
增大求解出来的方程斜率变小</li>
<li>Ridge回归源码解析：
<ul>
<li>alpha：正则项系数</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>solver：优化算法的选择</li>
</ul></li>
</ul>
<h4 id="lasso算法使用">3.2、Lasso算法使用</h4>
<p>这是scikit-learn官网给出的套索回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/b3deb13a7a55485d82e4417549bb8633.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>公式中多了一项：<span class="math inline">\(\frac{1}{2n_{samples}}\)</span>这是一个常数项，去掉之后，也不会影响损失函数公式计算。在岭回归中，就没有这项。</p>
<p>L1正则化和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">20</span>)</span><br><span class="line">w = np.random.randn(<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">lasso = Lasso(alpha= <span class="number">0.5</span>)</span><br><span class="line">lasso.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;套索回归求解的斜率：&#x27;</span>,lasso.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;套索回归求解的截距：&#x27;</span>,lasso.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>, l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知L1正则化，将方程系数进行了缩减，部分系数为0，产生稀疏模型</li>
<li><span class="math inline">\(\alpha\)</span>
越大，模型稀疏性越强，越多的参数为0</li>
<li>Lasso回归源码解析：
<ul>
<li>alpha：正则项系数</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>precompute：bool
类型，默认值为False，决定是否提前计算Gram矩阵来加速计算</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练</li>
</ul></li>
</ul>
<h4 id="elastic-net算法使用">3.3、Elastic-Net算法使用</h4>
<p>这是scikit-learn官网给出的弹性网络回归的，损失函数公式，注意，它用的矩阵表示，里面用到范数运算。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/91558aeaf118405085a6f799c1e2bcb4.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  Elastic-Net 回归，即岭回归和Lasso技术的混合。弹性网络是一种使用
L1， L2 范数作为先验正则项训练的线性回归模型。
这种组合允许学习到一个只有少量参数是非零稀疏的模型，就像 Lasso
一样，但是它仍然保持一些像 Ridge 的正则性质。我们可利用 l1_ratio
参数控制 L1 和 L2 的凸组合。</p>
<p>  弹性网络在很多特征互相联系（相关性，比如<strong>身高</strong>和<strong>体重</strong>就很有关系）的情况下是非常有用的。Lasso
很可能只随机考虑这些特征中的一个，而弹性网络更倾向于选择两个。</p>
<p>  在实践中，Lasso 和 Ridge 之间权衡的一个优势是它允许在迭代过程中继承
Ridge 的稳定性。</p>
<p>弹性网络回归和普通线性回归系数对比：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">20</span>)</span><br><span class="line">w = np.random.randn(<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的斜率：&#x27;</span>,w.ravel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;原始方程的截距：&#x27;</span>,b)</span><br><span class="line"></span><br><span class="line">model = ElasticNet(alpha= <span class="number">1</span>, l1_ratio = <span class="number">0.7</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;弹性网络回归求解的斜率：&#x27;</span>,model.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;弹性网络回归求解的截距：&#x27;</span>,model.intercept_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 线性回归梯度下降方法</span></span><br><span class="line">sgd = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=<span class="number">0</span>, l1_ratio=<span class="number">0</span>)</span><br><span class="line">sgd.fit(X, y.reshape(-<span class="number">1</span>,))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的斜率是：&#x27;</span>,sgd.coef_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;随机梯度下降求解的截距是：&#x27;</span>,sgd.intercept_)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>和没有正则项约束线性回归对比，可知Elastic-Net网络模型，融合了L1正则化L2正则化</li>
<li>Elastic-Net 回归源码解析：
<ul>
<li>alpha：混合惩罚项的常数</li>
<li>l1_ratio：弹性网混合参数，0 &lt;= l1_ratio &lt;= 1，对于 l1_ratio =
0，惩罚项是L2正则惩罚。对于 l1_ratio = 1是L1正则惩罚。对于 0</li>
<li>fit_intercept：是否计算 <span class="math inline">\(w_0\)</span>
截距项</li>
<li>normalize：是否做归一化</li>
<li>precompute：bool
类型，默认值为False，决定是否提前计算Gram矩阵来加速计算</li>
<li>max_iter：最大迭代次数</li>
<li>tol：结果的精确度</li>
<li>warm_start：bool类型，默认值为False。如果为True，那么使⽤用前⼀次训练结果继续训练。否则从头开始训练</li>
</ul></li>
</ul>
<h3 id="多项式回归">4、多项式回归</h3>
<h4 id="多项式回归基本概念">4.1、多项式回归基本概念</h4>
<p>  升维的目的是为了去解决欠拟合的问题的，也就是为了提高模型的准确率为目的的，因为当维度不够时，说白了就是对于预测结果考虑的因素少的话，肯定不能准确的计算出模型。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/4c50d109e36a4027bceef52123ba5a50.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  在做升维的时候，最常见的手段就是将已知维度进行相乘（或者自乘）来构建新的维度，如下图所示。普通线性方程，无法拟合规律，必须是多项式，才可以完美拟合曲线规律，图中是二次多项式。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/0cb4bee00c86449892903be1bfdda1f0.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  对于多项式回归来说主要是为了扩展线性回归算法来适应更广泛的数据集，比如我们数据集有两个维度
<span class="math inline">\(x_1、x_2\)</span>，那么用多元线性回归公式就是：<span class="math inline">\(\hat{y} = w_0 + w_1x_1 +
w_2x_2\)</span>，当我们使用二阶多项式升维的时候，数据集就从原来的 <span class="math inline">\(x_1、x_2\)</span>扩展成了<span class="math inline">\(x_1、x_2、x_1^2、x_2^2、x_1x_2\)</span>
。因此多元线性回归就得去多计算三个维度所对应的w值：<span class="math inline">\(\hat{y} = w_0 + w_1x_1 + w_2x_2 + w_3x_1^2 +
w_4x_2^2 + w_5x_1x_2\)</span> 。</p>
<p>  此时拟合出来的方程就是曲线，可以解决一些线性回归的欠拟合问题！</p>
<h4 id="多项式回归实战1.0">4.2、多项式回归实战1.0</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，并进行可视化</span></span><br><span class="line">X = np.linspace(-<span class="number">1</span>,<span class="number">11</span>,num = <span class="number">100</span>)</span><br><span class="line">y = (X - <span class="number">5</span>)**<span class="number">2</span> + <span class="number">3</span>*X -<span class="number">12</span> + np.random.randn(<span class="number">100</span>)</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、创建预测数据</span></span><br><span class="line">X_test = np.linspace(-<span class="number">2</span>,<span class="number">12</span>,num = <span class="number">200</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、不进行升维 + 普通线性回归</span></span><br><span class="line">model_1 = LinearRegression()</span><br><span class="line">model_1.fit(X,y)</span><br><span class="line">y_test_1 = model_1.predict(X_test)</span><br><span class="line">plt.plot(X_test,y_test,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、多项式升维 + 普通线性回归</span></span><br><span class="line">X = np.concatenate([X,X**<span class="number">2</span>],axis = <span class="number">1</span>)</span><br><span class="line">model_2 = LinearRegression()</span><br><span class="line">model_2.fit(X,y)</span><br><span class="line"><span class="comment"># 5、测试数据处理，并预测</span></span><br><span class="line">X_test = np.concatenate([X_test,X_test**<span class="number">2</span>],axis = <span class="number">1</span>)</span><br><span class="line">y_test_2 = model_2.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、数据可视化，切片操作</span></span><br><span class="line">plt.plot(X_test[:,<span class="number">0</span>],y_test_2,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>不进行多项式升维，拟合出来的曲线，是线性的直线，和目标曲线无法匹配</li>
<li>使用np.concatenate()进行简单的，幂次合并，注意数据合并的方向axis =
1</li>
<li>数据可视化时，注意切片，因为数据升维后，多了平方这一维</li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/d4264371bc9041d997ec84b09d77e6fe.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="多项式回归实战2.0">4.3、多项式回归实战2.0</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures,StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，并进行可视化</span></span><br><span class="line">X = np.linspace(-<span class="number">1</span>,<span class="number">11</span>,num = <span class="number">100</span>)</span><br><span class="line">y = (X - <span class="number">5</span>)**<span class="number">2</span> + <span class="number">3</span>*X -<span class="number">12</span> + np.random.randn(<span class="number">100</span>)</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.scatter(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、创建预测数据</span></span><br><span class="line">X_test = np.linspace(-<span class="number">2</span>,<span class="number">12</span>,num = <span class="number">200</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、使用PolynomialFeatures进行特征升维</span></span><br><span class="line">poly = PolynomialFeatures()</span><br><span class="line">poly.fit(X,y)</span><br><span class="line">X = poly.transform(X)</span><br><span class="line">s = StandardScaler()</span><br><span class="line">X = s.fit_transform(X)</span><br><span class="line"><span class="comment"># model = SGDRegressor(penalty=&#x27;l2&#x27;,eta0 = 0.0001,max_iter = 10000)</span></span><br><span class="line">model = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,eta0 = <span class="number">0.01</span>)</span><br><span class="line">model.fit(X,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、预测数据</span></span><br><span class="line">X_test = poly.transform(X_test)</span><br><span class="line">X_test_norm = s.transform(X_test)</span><br><span class="line">y_test = model.predict(X_test_norm)</span><br><span class="line">plt.plot(X_test[:,<span class="number">1</span>],y_test,color = <span class="string">&#x27;green&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>eta0表示学习率，设置合适的学习率，才能拟合成功</li>
<li>多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色</li>
<li>SGD随机梯度下降需要调整参数，以使模型适应数据</li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/8e8e751622bd4b10a6963673a8e3766b.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h3 id="代码实战天猫双十一销量预测">5、代码实战天猫双十一销量预测</h3>
<p>  天猫双十一，从2009年开始举办，第一届成交额仅仅0.5亿，后面呈现了爆发式的增长，那么这些增长是否有规律呢？是怎么样的规律，该如何分析呢？我们使用多项式回归一探究竟！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/31cb52d42edf43d6a46c382b4b40ce08.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>数据可视化，历年天猫双十一销量数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">18</span></span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据，年份数据2009 ~ 2019</span></span><br><span class="line">X = np.arange(<span class="number">2009</span>,<span class="number">2020</span>)</span><br><span class="line">y = np.array([<span class="number">0.5</span>,<span class="number">9.36</span>,<span class="number">52</span>,<span class="number">191</span>,<span class="number">350</span>,<span class="number">571</span>,<span class="number">912</span>,<span class="number">1207</span>,<span class="number">1682</span>,<span class="number">2135</span>,<span class="number">2684</span>])</span><br><span class="line">plt.bar(X,y,width = <span class="number">0.5</span>,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.plot(X,y,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">_ = plt.xticks(ticks = X)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/10d4dd476d8e40daba21a97d77866da8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>有图可知，在一定时间内，随着经济的发展，天猫双十一销量与年份的关系是多项式关系！假定，销量和年份之间关系是三次幂关系：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/35f0479254104929b16b08ebb713fbc6.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">9</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据，年份数据2009 ~ 2019</span></span><br><span class="line">X = np.arange(<span class="number">2009</span>,<span class="number">2020</span>)</span><br><span class="line">y = np.array([<span class="number">0.5</span>,<span class="number">9.36</span>,<span class="number">52</span>,<span class="number">191</span>,<span class="number">350</span>,<span class="number">571</span>,<span class="number">912</span>,<span class="number">1207</span>,<span class="number">1682</span>,<span class="number">2135</span>,<span class="number">2684</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、年份数据，均值移除，防止某一个特征列数据天然的数值太大而影响结果</span></span><br><span class="line">X = X - X.mean()</span><br><span class="line">X = X.reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、构建多项式特征，3次幂</span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">3</span>)</span><br><span class="line">X = poly.fit_transform(X)</span><br><span class="line">s = StandardScaler()</span><br><span class="line">X_norm = s.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建模型</span></span><br><span class="line">model = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,eta0 = <span class="number">0.5</span>,max_iter = <span class="number">5000</span>)</span><br><span class="line">model.fit(X_norm,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、数据预测</span></span><br><span class="line">X_test = np.linspace(-<span class="number">5</span>,<span class="number">6</span>,<span class="number">100</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">X_test = poly.transform(X_test)</span><br><span class="line">X_test_norm = s.transform(X_test)</span><br><span class="line">y_test = model.predict(X_test_norm)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、数据可视化</span></span><br><span class="line">plt.plot(X_test[:,<span class="number">1</span>],y_test,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.bar(X[:,<span class="number">1</span>],y)</span><br><span class="line">plt.bar(<span class="number">6</span>,y_test[-<span class="number">1</span>],color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">4096</span>)</span><br><span class="line">plt.text(<span class="number">6</span>,y_test[-<span class="number">1</span>] + <span class="number">100</span>,<span class="built_in">round</span>(y_test[-<span class="number">1</span>],<span class="number">1</span>),ha = <span class="string">&#x27;center&#x27;</span>)</span><br><span class="line">_ = plt.xticks(np.arange(-<span class="number">5</span>,<span class="number">7</span>),np.arange(<span class="number">2009</span>,<span class="number">2021</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/6a839d47304f4373b84be460374381a5.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>结论：</strong></p>
<ul>
<li>数据预处理，均值移除。如果特征<strong>基准值和分散度</strong>不同在某些算法（例如回归算法，KNN等）上可能会大大影响了模型的预测能力。通过均值移除，大大增强数据的<strong>离散化</strong>程度。</li>
<li>多项式升维，需要对数据进行Z-score归一化处理，效果更佳出色</li>
<li>SGD随机梯度下降需要调整参数，以使模型适应多项式数据</li>
<li>从2020年开始，天猫双十一统计的成交额改变了规则为11.1日~11.11日的成交数据（之前的数据为双十一当天的数据），2020年成交额为<strong>4980</strong>亿元</li>
<li>可以，经济发展有其客观规律，前11年高速发展（曲线基本可以反应销售规律），到2020年是一个转折点</li>
</ul>
<h3 id="代码实战中国人寿保费预测">6、代码实战中国人寿保费预测</h3>
<h4 id="数据加载与介绍">6.1、数据加载与介绍</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_excel(<span class="string">&#x27;./中国人寿.xlsx&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<p>数据介绍：</p>
<ul>
<li>共计1338条保险数据，每条数据7个属性</li>
<li>最后一列charges是保费</li>
<li>前面6列是特征，分别为：年龄、性别、体重指数、小孩数量、是否抽烟、所在地区</li>
</ul>
<h4 id="eda数据探索">6.2、EDA数据探索</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="comment"># 性别对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;sex&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 地区对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;region&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 吸烟对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;smoker&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 孩子数量对保费影响</span></span><br><span class="line">sns.kdeplot(data[<span class="string">&#x27;charges&#x27;</span>],shade = <span class="literal">True</span>,hue = data[<span class="string">&#x27;children&#x27;</span>],palette=<span class="string">&#x27;Set1&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652344917096/93b4a181835c4083a9e43c3383583369.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>总结：</p>
<ul>
<li>不同性别对保费影响不大，不同性别的保费的概率分布曲线基本重合，因此这个特征无足轻重，可以删除</li>
<li>地区同理</li>
<li>吸烟与否对保费的概率分布曲线差别很大，整体来说不吸烟更加健康，那么保费就低，这个特征很重要</li>
<li>家庭孩子数量对保费有一定影响</li>
</ul>
<h4 id="特征工程">6.3、特征工程</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">&#x27;region&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">data.head() <span class="comment"># 删除不重要特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 体重指数，离散化转换，体重两种情况：标准、肥胖</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert</span>(<span class="params">df,bmi</span>):</span><br><span class="line">    df[<span class="string">&#x27;bmi&#x27;</span>] = <span class="string">&#x27;fat&#x27;</span> <span class="keyword">if</span> df[<span class="string">&#x27;bmi&#x27;</span>] &gt;= bmi <span class="keyword">else</span> <span class="string">&#x27;standard&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line">data = data.apply(convert, axis = <span class="number">1</span>, args=(<span class="number">30</span>,))</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征提取，离散型数据转换为数值型数据</span></span><br><span class="line">data = pd.get_dummies(data)</span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征和目标值抽取</span></span><br><span class="line">X = data.drop(<span class="string">&#x27;charges&#x27;</span>, axis=<span class="number">1</span>) <span class="comment"># 训练数据</span></span><br><span class="line">y = data[<span class="string">&#x27;charges&#x27;</span>] <span class="comment"># 目标值</span></span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure>
<h4 id="特征升维">6.4、特征升维</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,mean_squared_log_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据拆分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征升维</span></span><br><span class="line">poly = PolynomialFeatures(degree= <span class="number">2</span>, include_bias = <span class="literal">False</span>)</span><br><span class="line">X_train_poly = poly.fit_transform(X_train)</span><br><span class="line">X_test_poly = poly.fit_transform(X_test)</span><br></pre></td></tr></table></figure>
<h4 id="模型训练与评估">6.5、模型训练与评估</h4>
<p>普通线性回归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_1 = LinearRegression()</span><br><span class="line">model_1.fit(X_train_poly, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据得分：&#x27;</span>,model_1.score(X_train_poly,y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测数据得分：&#x27;</span>,model_1.score(X_test_poly,y_test))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据均方误差：&#x27;</span>,np.sqrt(mean_squared_error(y_train,model_1.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据均方误差：&#x27;</span>,np.sqrt(mean_squared_error(y_test,model_1.predict(X_test_poly))))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据对数误差：&#x27;</span>,np.sqrt(mean_squared_log_error(y_train,model_1.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据对数误差：&#x27;</span>,np.sqrt(mean_squared_log_error(y_test,model_1.predict(X_test_poly))))</span><br></pre></td></tr></table></figure>
<p>弹性网络回归：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model_2 = ElasticNet(alpha = <span class="number">0.3</span>,l1_ratio = <span class="number">0.5</span>,max_iter = <span class="number">50000</span>)</span><br><span class="line">model_2.fit(X_train_poly,y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据得分：&#x27;</span>,model_2.score(X_train_poly,y_train))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;预测数据得分：&#x27;</span>,model_2.score(X_test_poly,y_test))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据均方误差为：&#x27;</span>,np.sqrt(mean_squared_error(y_train,model_2.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据均方误差为：&#x27;</span>,np.sqrt(mean_squared_error(y_test,model_2.predict(X_test_poly))))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据对数误差为：&#x27;</span>,np.sqrt(mean_squared_log_error(y_train,model_2.predict(X_train_poly))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据对数误差为：&#x27;</span>,np.sqrt(mean_squared_log_error(y_test,model_2.predict(X_test_poly))))</span><br></pre></td></tr></table></figure>
<p><strong>结论：</strong></p>
<ul>
<li>进行EDA数据探索，可以查看无关紧要特征</li>
<li>进行特征工程：删除无用特征、特征离散化、特征提取。这对机器学习都至关重要</li>
<li>对于简单的数据（特征比较少）进行线性回归，一般需要进行特征升维</li>
<li>选择不同的算法，进行训练和评估，从中筛选优秀算法</li>
</ul>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2023-08-17</span>
            
                <span>该篇文章被 albert dong</span>
            
            
             
                <span>归为分类:
                    
                    
                        <a href='../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/'>
                            机器学习
                        </a>
                    
                </span>
            
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>上一篇：<a href='../%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/'>多元线性回归</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">下一篇：<a href="../../../../06/17/%E6%B6%88%E6%81%AF/kafka/">kafka</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            © 1949-2024 China 

            
                

            
        </span>
       
    
</div>



<!--这是指一条线往下的内容-->
<div class="footer-last">
    
            <span>🌊看过大海的人不会忘记海的广阔🌊</span>
            
                <span class="footer-last-span-right"><i>本站由<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>驱动｜使用<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>主题</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--目录-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="../../../../../js/toc.js"></script>

    

    
<script src="../../../../../js/randomHeaderContent.js"></script>

    <!--回到顶部按钮-->
    
        
<script src="../../../../../js/returnToTop.js"></script>

    

    
        
<script src="../../../../../js/returnToLastPage.js"></script>

    





<script src="../../../../../js/lightgallery/lightgallery.umd.min.js"></script>



<script src="../../../../../js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="../../../../../js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="../../../../../js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="../../../../../js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="../../../../../js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="../../../../../js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // 启用插件
            thumbnail: true,          // 显示缩略图
            zoom: true,               // 启用缩放功
            rotate: true,             // 启用旋转功能能
            autoplay: true,        // 启用自动播放功能
            fullScreen: true,      // 启用全屏功能
            pager: false, //页码,
            zoomFromOrigin: true,   // 从原始位置缩放
            actualSize: true,       // 启用查看实际大小的功能
            enableZoomAfter: 300,    // 延迟缩放，确保图片加载完成后可缩放
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // 修复选择器
    }
    
</script>


    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> 

                </div>
            
            
                <!-- 回到顶部的按钮-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- 返回的按钮-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>