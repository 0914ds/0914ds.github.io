<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>多元线性回归 - blog</title><link rel="manifest" href="../../../../../manifest.json"><meta name="application-name" content="blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="梯度下降 线性回归预测房价  数据加载 数据介绍 数据拆分 数据建模 数据预测 数据评估  1、无约束最优化问题 1.1、无约束最优化    无约束最优化问题（unconstrained optimization problem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函\(J(\th"><meta property="og:type" content="blog"><meta property="og:title" content="多元线性回归"><meta property="og:url" content="https://0914ds.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><meta property="og:site_name" content="blog"><meta property="og:description" content="梯度下降 线性回归预测房价  数据加载 数据介绍 数据拆分 数据建模 数据预测 数据评估  1、无约束最优化问题 1.1、无约束最优化    无约束最优化问题（unconstrained optimization problem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函\(J(\th"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ac185dc9b60d4411b6d0c9ae9dc15543.jpg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/6e97ba09e2694bf69d2919cd159a6a59.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/680d3e456f084caa87d35c55ece68b98.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/22d31ba2d7c348bca74593141837f569.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/66ce3adba54149a382512476bf1da237.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9f2ec32dd16d40ceb75d032cbe6d3ea9.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83476c288f4d4e84951997fc776a8fa1.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c782a1c81fb240c89ef39bf55d74d3e1.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/7d59daf9da474ef9b1716191f47064c8.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/51536cae3e524a419ab6feff64e5df31.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9d04508431a94e4f9a54bed4c6e01dc8.jpg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c3a7bf82304d4cd990306bccc9e33e98.jpg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/29a4d4047fa24ea495f55630a6c6d174.jpg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83b78948c0074a17b4334e1ca691b8fc.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/49c3dfaf8c4b4a54ab80a7e51f90cfa6.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/670e2e063398461e9143ae13b7c4709a.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/038487f53e8f4c47af75de156a2e98c4.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/1e745bd465d542cc8593e12977a6cb26.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/bd716fec62c2485e831263b5d84f8197.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/52e7aec970ae4be887f2b6eb059e5a54.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f983d092c8914a60876c80853bd08b43.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/b23d78b73e8c4a88813c02917ff33853.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/4e0a2d7309a2482882c7c641014a98af.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/25e11b29bcdc48df880924c7e75c1b16.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/608f1025518842188c8c47350e5e8ce8.png"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/fa690fe760284383b198810802e82995.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/0bbe5b04ddf24a78a71c4700cbde90a5.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/da9eec25972249f1a587a10d5723320f.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/def103855dc34212bd5cfda1686a73bb.jpeg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f95707a6d51948c3a6aa7fcb61b5c055.jpg"><meta property="og:image" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ab1758d6f7504c128eee9bdb71045e4d.png"><meta property="article:published_time" content="2023-08-17T17:13:16.000Z"><meta property="article:modified_time" content="2023-08-17T17:30:11.349Z"><meta property="article:author" content="albert dong"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ac185dc9b60d4411b6d0c9ae9dc15543.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://0914ds.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"},"headline":"多元线性回归","image":["https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ac185dc9b60d4411b6d0c9ae9dc15543.jpg","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/680d3e456f084caa87d35c55ece68b98.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/66ce3adba54149a382512476bf1da237.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/7d59daf9da474ef9b1716191f47064c8.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9d04508431a94e4f9a54bed4c6e01dc8.jpg","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c3a7bf82304d4cd990306bccc9e33e98.jpg","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/29a4d4047fa24ea495f55630a6c6d174.jpg","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/49c3dfaf8c4b4a54ab80a7e51f90cfa6.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/670e2e063398461e9143ae13b7c4709a.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/038487f53e8f4c47af75de156a2e98c4.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/1e745bd465d542cc8593e12977a6cb26.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/bd716fec62c2485e831263b5d84f8197.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/52e7aec970ae4be887f2b6eb059e5a54.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f983d092c8914a60876c80853bd08b43.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/4e0a2d7309a2482882c7c641014a98af.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/608f1025518842188c8c47350e5e8ce8.png","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f95707a6d51948c3a6aa7fcb61b5c055.jpg","https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ab1758d6f7504c128eee9bdb71045e4d.png"],"datePublished":"2023-08-17T17:13:16.000Z","dateModified":"2023-08-17T17:30:11.349Z","author":{"@type":"Person","name":"albert dong"},"publisher":{"@type":"Organization","name":"blog","logo":{"@type":"ImageObject","url":"https://0914ds.github.io/img/logo.svg"}},"description":"梯度下降\r 线性回归预测房价\r \r 数据加载\r 数据介绍\r 数据拆分\r 数据建模\r 数据预测\r 数据评估\r \r 1、无约束最优化问题\r 1.1、无约束最优化\r    无约束最优化问题（unconstrained optimization\r problem）指的是从一个问题的所有可能的备选方案中，选择出依某种指标来说是最优的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函\\(J(\\th"}</script><link rel="canonical" href="https://0914ds.github.io/2023/08/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"><link rel="icon" href="../../../../../img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="../../../../../css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="atom.xml" title="blog" type="application/atom+xml">
</head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="../../../../../index.html"><img src="../../../../../img/logo.svg" alt="blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="../../../../../index.html">Home</a><a class="navbar-item" href="../../../../../archives">Archives</a><a class="navbar-item" href="../../../../../categories">Categories</a><a class="navbar-item" href="../../../../../tags">Tags</a><a class="navbar-item" href="../../../../../about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-17T17:13:16.000Z" title="2023/8/18 01:13:16">2023-08-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-17T17:30:11.349Z" title="2023/8/18 01:30:11">2023-08-18</time></span><span class="level-item"><a class="link-muted" href="../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span><span class="level-item">an hour read (About 7909 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">多元线性回归</h1><div class="content"><meta name="referrer" content="no-referrer">
<h2 id="梯度下降">梯度下降</h2>
<h3 id="线性回归预测房价">线性回归预测房价</h3>
<ul>
<li>数据加载</li>
<li>数据介绍</li>
<li>数据拆分</li>
<li>数据建模</li>
<li>数据预测</li>
<li>数据评估</li>
</ul>
<h3 id="无约束最优化问题">1、无约束最优化问题</h3>
<h4 id="无约束最优化">1.1、无约束最优化</h4>
<p> <strong>  无约束最优化问题</strong>（unconstrained optimization
problem）指的是从一个问题的所有<strong>可能</strong>的备选方案中，选择出依某种指标来说是<strong>最优</strong>的解决方案。从数学上说，最优化是研究在一个给定的集合S上泛函<span class="math inline">\(J(\theta)\)</span>的极小化或极大化问题：<strong>广义上</strong>，最优化包括数学规划、图和网络、组合最优化、库存论、决策论、排队论、最优控制等。<strong>狭义上</strong>，最优化仅指数学规划。</p>
<h4 id="梯度下降-1">1.2、梯度下降</h4>
<p>  <strong>梯度下降法</strong>(Gradient
Descent)是一个算法，但不是像多元线性回归那样是一个具体做回归任务的算法，而是一个非常<strong>通用</strong>的优化算法来帮助一些机器学习算法（都是无约束最优化问题）求解出<strong>最优解</strong>，
所谓的通用就是很多机器学习算法都是用梯度下降，甚至<strong>深度学习</strong>也是用它来求解最优解。所有优化算法的目的都是期望以<strong>最快</strong>的速度把模型参数θ求解出来，梯度下降法就是一种<strong>经典</strong>常用的优化算法。</p>
<p>  之前利用正规方程求解的 θ 是最优解的原因是 MSE
这个损失函数是凸函数。但是，机器学习的损失函数并非都是凸函数，设置导数为
0 会得到很多个极值，不能确定唯一解。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ac185dc9b60d4411b6d0c9ae9dc15543.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  使用正规方程 <span class="math inline">\(\theta =
(X^TX)^{-1}X^Ty\)</span> 求解的另一个限制是特征维度（<span class="math inline">\(X_1、X_2……、X_n\)</span>）不能太多，矩阵逆运算的时间复杂度通常为
<span class="math inline">\(O(n^3)\)</span>
。换句话说，就是如果特征数量翻$
2^3$倍，你的计算时间大致为原来的倍，也就是之前时间的8倍。举个例子，2
个特征 1 秒，4 个特征就是 8 秒，8 个特征就是 64 秒，16 个特征就是 512
秒，当特征更多的时候呢？运行时间会非常漫长~</p>
<p>  所以正规方程求出最优解<strong>并不是</strong>机器学习甚至深度学习常用的手段。</p>
<p>  之前我们令导数为 0，反过来求解最低点 θ
是多少，而梯度下降法是<strong>一点点</strong>去逼近最优解!</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/6e97ba09e2694bf69d2919cd159a6a59.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  其实这就跟生活中的情形很像，比如你问一个朋友的工资是多少，他说你猜？那就很难了，他说你猜完我告诉你是猜高了还是猜低了，这样你就可以奔着对的方向一直猜下去，最后总会猜对！梯度下降法就是这样的，多次尝试。并且，在试的过程中还得想办法知道是不是在猜对的路上，说白了就是得到正确的反馈再调整然后继续猜才有意义~</p>
<p>  这个就好比道士下山，我们把 Loss
（或者称为Cost，即损失）曲线看成是<strong>山谷</strong>，如果走过了，就再
往回返，所以是一个迭代的过程。</p>
<h4 id="梯度下降公式">1.3、梯度下降公式</h4>
<p>  这里梯度下降法的公式就是一个式子指导计算机迭代过程中如何去调整<span class="math inline">\(\theta\)</span>，可以通过泰勒公式一阶展开来进行推导和证明：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/680d3e456f084caa87d35c55ece68b98.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  这里的 <span class="math inline">\(w_j\)</span> 就是 <span class="math inline">\(\theta\)</span> 中的某一个 j = 0...m，这里的 <span class="math inline">\(\eta\)</span> 就是梯度下降图里的 learning
step，很多时候也叫学习率 learning rate，很多时候也用 <span class="math inline">\(\alpha\)</span>
表示，这个学习率我们可以看作是下山迈的<strong>步子</strong>的大小，步子迈的大下山就快。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/22d31ba2d7c348bca74593141837f569.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  学习率一般都是<strong>正数</strong>，如果在山左侧（曲线<strong>左半边</strong>）梯度是负的，那么这个负号就会把
<span class="math inline">\(w_j\)</span> 往大了调，
如果在山右侧（曲线右半边）梯度就是正的，那么负号就会把 <span class="math inline">\(w_j\)</span> 往小了调。每次 <span class="math inline">\(w_j\)</span> 调整的幅度就是 <span class="math inline">\(\eta *
gradient\)</span>，就是横轴上移动的距离。</p>
<p>  因此，无论在左边，还是在右边，梯度下降都可以快速找到最优解，实现快速<strong>下山</strong>~</p>
<p>  如果特征或维度越多，那么这个公式用的次数就越多，也就是每次迭代要应用的这个式子多次（多少特征，就应用多少次），所以其实上面的图不是特别准，因为
<span class="math inline">\(\theta\)</span>
对应的是很多维度，应该每一个维度都可以画一个这样的图，或者是一个多维空间的图。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/66ce3adba54149a382512476bf1da237.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9f2ec32dd16d40ceb75d032cbe6d3ea9.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  所以观察上图我们可以发现不是某一个 <span class="math inline">\(\theta_0\)</span> 或 <span class="math inline">\(\theta_1\)</span>
找到最小值就是最优解，而是它们一起找到 <span class="math inline">\(J(\theta)\)</span> 最小值才是最优解。</p>
<h4 id="学习率">1.4、学习率</h4>
<p>  根据我们上面讲的梯度下降公式，我们知道 <span class="math inline">\(\eta\)</span> 是学习率，设置大的学习率 <span class="math inline">\(w_j\)</span> 每次调整的幅度就大，设置小的学习率
<span class="math inline">\(w_j\)</span>
每次调整的幅度就小，然而如果步子迈的太大也会有问题，俗话说步子大了容易扯着蛋！学习率大，可能一下子迈过了，到另一边去了（从曲线左半边跳到右半边），继续梯度下降又迈回来，
使得来来回回震荡。步子太小呢，就像蜗牛一步步往前挪，也会使得整体迭代次数增加。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83476c288f4d4e84951997fc776a8fa1.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  学习率的设置是门一门学问，一般我们会把它设置成一个比较小的正整数，0.1、0.01、0.001、0.0001，都是常见的设定数值（然后根据情况调整）。一般情况下学习率在整体迭代过程中是不变，但是也可以设置成随着迭代次数增多学习率逐渐变小，因为越靠近山谷我们就可以步子迈小点，可以更精准的走入最低点，同时防止走过。还有一些深度学习的优化算法会自己控制调整学习率这个值，后面学习过程中这些策略在讲解代码中我们会一一讲到。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c782a1c81fb240c89ef39bf55d74d3e1.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="全局最优化">1.5、全局最优化</h4>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/7d59daf9da474ef9b1716191f47064c8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>上图显示了梯度下降的两个主要挑战：</p>
<ul>
<li>若随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值；</li>
<li>若随机初始化，算法从右侧起步，那么需要经过很长时间才能越过Plateau（函数停滞带，梯度很小），如果停下得太早，则永远达不到全局最小值；</li>
</ul>
<p>  而线性回归的模型MSE损失函数恰好是个凸函数，凸函数保证了只有一个全局最小值，其次是个连续函数，斜率不会发生陡峭的变化，因此即便是乱走，梯度下降都可以趋近全局最小值。</p>
<p>  上图损失函数是非凸函数，梯度下降法是有可能落到局部最小值的，所以其实步长不能设置的太小太稳健，那样就很容易落入局部最优解，虽说局部最小值也没大问题，
因为模型只要是<strong>堪用</strong>的就好嘛，但是我们肯定还是尽量要奔着全局最优解去！</p>
<h4 id="梯度下降步骤">1.6、梯度下降步骤</h4>
<p>梯度下降流程就是“猜”正确答案的过程:</p>
<ul>
<li><p>1、“瞎蒙”，Random 随机数生成 <span class="math inline">\(\theta\)</span>，随机生成一组数值 <span class="math inline">\(w_0、w_1……w_n\)</span> ，期望 <span class="math inline">\(\mu\)</span> 为 0 方差 <span class="math inline">\(\sigma\)</span> 为 1 的正太分布数据。</p></li>
<li><p>2、求梯度 g
，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p></li>
<li><p>3、if g &lt; 0, <span class="math inline">\(\theta\)</span>
变大，if g &gt; 0, <span class="math inline">\(\theta\)</span>
变小</p></li>
<li><p>4、判断是否收敛，如果收敛跳出迭代，如果没有达到收敛，回第 2
步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p></li>
</ul>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/51536cae3e524a419ab6feff64e5df31.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="代码模拟梯度下降">1.7、代码模拟梯度下降</h4>
<ul>
<li><p>梯度下降优化算法，比正规方程，应用更加广泛</p></li>
<li><p>什么是梯度？</p>
<ul>
<li>梯度就是导数对应的值！</li>
</ul></li>
<li><p>下降？</p>
<ul>
<li>涉及到优化问题，最小二乘法</li>
</ul></li>
<li><p>梯度下降呢？</p>
<ul>
<li>梯度方向下降，速度最快的~</li>
</ul></li>
</ul>
<p>  接下来，我们使用代码来描述上面梯度下降的过程：</p>
<p>方程如下：</p>
<p><span class="math inline">\(f(x) = (x - 3.5)^2 - 4.5x +
10\)</span></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/9d04508431a94e4f9a54bed4c6e01dc8.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>使用梯度下降的思想，来一步步逼近，函数的最小值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">f = <span class="keyword">lambda</span> x : (x - <span class="number">3.5</span>)**<span class="number">2</span> -<span class="number">4.5</span>*x + <span class="number">10</span></span><br><span class="line"><span class="comment"># 导函数</span></span><br><span class="line">d = <span class="keyword">lambda</span> x :<span class="number">2</span>*(x - <span class="number">3.5</span>) - <span class="number">4.5</span> <span class="comment"># 梯度 == 导数</span></span><br><span class="line"><span class="comment"># 梯度下降的步幅，比例，（学习率，幅度）</span></span><br><span class="line">step = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 求解当x等于多少的时候，函数值最小。求解目标值：随机生成的</span></span><br><span class="line"><span class="comment"># 相等于：&#x27;瞎蒙&#x27; ----&gt; 方法 ----&gt; 优化</span></span><br><span class="line">x = np.random.randint(<span class="number">0</span>,<span class="number">12</span>,size = <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 梯度下降，每下降一步，每走一步，目标值，都会更新。</span></span><br><span class="line"><span class="comment"># 更新的这个新值和上一步的值，差异，如果差异很小（万分之一）</span></span><br><span class="line"><span class="comment"># 梯度下降退出</span></span><br><span class="line">last_x = x + <span class="number">0.02</span> <span class="comment"># 记录上一步的值，首先让last_x和x有一定的差异！！！</span></span><br><span class="line"><span class="comment"># 精确率，真实计算，都是有误差，自己定义</span></span><br><span class="line">precision = <span class="number">1e-4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;+++++++++++++++++++++&#x27;</span>, x)</span><br><span class="line">x_ = [x]</span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="comment"># 退出条件，精确度，满足了</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">abs</span>(x - last_x) &lt; precision:</span><br><span class="line">        <span class="keyword">break</span>   </span><br><span class="line">    <span class="comment"># 更新</span></span><br><span class="line">    last_x = x</span><br><span class="line">    x -= step*d(x) <span class="comment"># 更新，减法：最小值</span></span><br><span class="line">    x_.append(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------&#x27;</span>,x)</span><br><span class="line"><span class="comment"># 数据可视化</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.family&#x27;</span>] = <span class="string">&#x27;Kaiti SC&#x27;</span></span><br><span class="line">plt.figure(figsize=(<span class="number">9</span>,<span class="number">6</span>))</span><br><span class="line">x = np.linspace(<span class="number">5.75</span> - <span class="number">5</span>, <span class="number">5.75</span> + <span class="number">5</span>, <span class="number">100</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">plt.plot(x,y,color = <span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;梯度下降&#x27;</span>,size = <span class="number">24</span>,pad = <span class="number">15</span>)</span><br><span class="line">x_ = np.array(x_)</span><br><span class="line">y_ = f(x_)</span><br><span class="line">plt.scatter(x_, y_,color = <span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.savefig(<span class="string">&#x27;./图片/5-梯度下降.jpg&#x27;</span>,dpi = <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<p>函数的最优解是：<strong>5.75</strong>。你可以发现，随机赋值的变量 x
，无论<strong>大于</strong>5.75，还是<strong>小于</strong>5.75，经过梯度下降，最终都慢慢靠近5.75这个最优解！</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/c3a7bf82304d4cd990306bccc9e33e98.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/29a4d4047fa24ea495f55630a6c6d174.jpg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>注意：</strong></p>
<ol type="1">
<li>梯度下降存在一定误差，不是完美解~</li>
<li>在误差允许的范围内，梯度下降所求得的机器学习模型，是堪用的！</li>
<li>梯度下降的步幅step，不能太大，俗话说步子不能迈的太大！</li>
<li>精确度，可以根据实际情况调整</li>
<li>while True循环里面，持续进行梯度下降：</li>
</ol>
<p>   <span class="math inline">\(\theta = \theta - \eta
\frac{\partial}{\partial \theta}J(\theta)\)</span> 其中的 $$
叫做学习率</p>
<p>  <span class="math inline">\(x = x - \eta\frac{\partial}{\partial
x}f(x)\)</span></p>
<p>  <span class="math inline">\(x = x - step*\frac{\partial}{\partial
x} f(x)\)</span> 其中的 $step $ 叫做学习率</p>
<p>  <span class="math inline">\(x = x - step * f&#39;(x)\)</span></p>
<ol start="6" type="1">
<li>while
循环退出条件是：x更新之后和上一次相差绝对值小于特定精确度！</li>
</ol>
<h3 id="梯度下降方法">2、梯度下降方法</h3>
<h4 id="三种梯度下降不同">2.1、三种梯度下降不同</h4>
<p>梯度下降分三类：批量梯度下降BGD（<strong>Batch Gradient
Descent</strong>）、小批量梯度下降MBGD（<strong>Mini-Batch Gradient
Descent</strong>）、随机梯度下降SGD（<strong>Stochastic Gradient
Descent</strong>）。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/83b78948c0074a17b4334e1ca691b8fc.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>三种梯度下降有什么不同呢？我们从梯度下降步骤开始讲起，梯度下降步骤分一下四步：</p>
<ul>
<li><p>1、随机赋值，Random 随机数生成 <span class="math inline">\(\theta\)</span>，随机一组数值 <span class="math inline">\(w_0、w_1……w_n\)</span></p></li>
<li><p>2、求梯度 g
，梯度代表曲线某点上的切线的斜率，沿着切线往下就相当于沿着坡度最陡峭的方向下降</p></li>
<li><p>3、if g &lt; 0, <span class="math inline">\(\theta\)</span>
变大，if g &gt; 0, <span class="math inline">\(\theta\)</span>
变小</p></li>
<li><p>4、判断是否收敛
convergence，如果收敛跳出迭代，如果没有达到收敛，回第 2
步再次执行2~4步</p>
<p>收敛的判断标准是：随着迭代进行损失函数Loss，变化非常微小甚至不再改变，即认为达到收敛</p></li>
</ul>
<p>三种梯度下降不同，体现在第二步中：</p>
<ul>
<li>BGD是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新</li>
<li>MBGD是指在<strong>每次迭代</strong>使用<strong>一部分样本</strong>（所有样本500个，使用其中32个样本）来进行梯度的更新</li>
<li>SGD是指<strong>每次迭代</strong>随机选择<strong>一个样本</strong>来进行梯度更新</li>
</ul>
<h4 id="线性回归梯度更新公式">2.2、线性回归梯度更新公式</h4>
<p>回顾上一讲公式！</p>
<p>最小二乘法公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/49c3dfaf8c4b4a54ab80a7e51f90cfa6.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>矩阵写法：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/670e2e063398461e9143ae13b7c4709a.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>接着我们来讲解如何求解上面梯度下降的第 2
步，即我们要推导出损失函数的导函数来。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/038487f53e8f4c47af75de156a2e98c4.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>  <span class="math inline">\(x^2\)</span>的导数就是
2x，根据链式求导法则，我们可以推出上面第（1）步。然后是多元线性回归，所以
<span class="math inline">\(h_{\theta}(x)\)</span> 就 是 <span class="math inline">\(\theta^Tx\)</span> 即是<span class="math inline">\(w_0x_0 + w_1x_1 + …… + w_nx_n\)</span> 即<span class="math inline">\(\sum\limits_{i =
0}^n\theta_ix_i\)</span>。到这里我们是对 <span class="math inline">\(\theta_j\)</span> 来求偏导，那么和 <span class="math inline">\(w_j\)</span> 没有关系的可以忽略不计，所以只剩下
<span class="math inline">\(x_j\)</span>。</p>
<p>  我们可以得到结论就是 <span class="math inline">\(\theta_j\)</span>
对应的梯度与预测值 <span class="math inline">\(\hat{y}\)</span> 和真实值
y 有关，这里 <span class="math inline">\(\hat{y}\)</span> 和 y
是列向量（即多个数据），同时还与 <span class="math inline">\(\theta_j\)</span> 对应的特征维度 <span class="math inline">\(x_j\)</span> 有关，这里 <span class="math inline">\(x_j\)</span> 是原始数据集矩阵的第 j
列。如果我们分别去对每个维度 <span class="math inline">\(\theta_0、\theta_1……\theta_n\)</span>
求偏导，即可得到所有维度对应的梯度值。</p>
<ul>
<li><span class="math inline">\(g_0 = (h_{\theta}(x) -
y)x_0\)</span></li>
<li><span class="math inline">\(g_1 = (h_{\theta}(x) -
y)x_1\)</span></li>
<li>……</li>
<li><span class="math inline">\(g_j = (h_{\theta}(x) -
y)x_j\)</span></li>
</ul>
<p><strong>总结：</strong></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/1e745bd465d542cc8593e12977a6cb26.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="批量梯度下降bgd">2.3、批量梯度下降BGD</h4>
<p>  <strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每次迭代</strong>使用<strong>所有样本</strong>来进行梯度的更新。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/bd716fec62c2485e831263b5d84f8197.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>去掉 <span class="math inline">\(\frac{1}{n}\)</span>
也可以，因为它是一个常量，可以和 <span class="math inline">\(\eta\)</span> 合并</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/52e7aec970ae4be887f2b6eb059e5a54.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>矩阵写法：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f983d092c8914a60876c80853bd08b43.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>其中 𝑖 = 1, 2, ..., n 表示样本数， 𝑗 = 0,
1……表示特征数，<strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} = 1\)</span></strong>。</p>
<p><strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理！</strong></p>
<p><strong>优点：</strong>
  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。
<strong>缺点：</strong>   （1）当样本数目 n
很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。</p>
<p>从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/b23d78b73e8c4a88813c02917ff33853.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="随机梯度下降sgd">2.4、随机梯度下降SGD</h4>
<p><strong>随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/4e0a2d7309a2482882c7c641014a98af.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p><strong>批量梯度下降</strong>算法每次都会使用<strong>全部</strong>训练样本，因此这些计算是冗余的，因为每次都使用完全相同的样本集。而<strong>随机梯度下降</strong>算法每次只随机选择<strong>一个</strong>样本来更新模型参数，因此每次的学习是非常快速的。</p>
<p>  <strong>优点：</strong>
  （1）由于不是在全部训练数据上的更新计算，而是在每轮迭代中，随机选择一条数据进行更新计算，这样每一轮参数的更新速度大大加快。
  <strong>缺点：</strong>
  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。</p>
<p>  <strong>解释一下为什么SGD收敛速度比BGD要快：</strong>   *
这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）。
  *
而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被迭代30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。
  * 也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W
次。</p>
<p>从迭代的次数上来看，SGD迭代的次数较多，在解空间的搜索过程就会盲目一些。其迭代的收敛曲线示意图可以表示如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/25e11b29bcdc48df880924c7e75c1b16.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="小批量梯度下降mbgd">2.5、小批量梯度下降MBGD</h4>
<p><strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个<strong>折中</strong>办法。其思想是：<strong>每次迭代</strong>使用总样本中的一部分（batch_size）样本来对参数进行更新。这里我们假设
batch_size = 32，样本数 n = 1000
。实现了更新速度与更新次数之间的平衡。每次迭代参数更新公式如下：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/608f1025518842188c8c47350e5e8ce8.png" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<p>相对于随机梯度下降算法，小批量梯度下降算法降低了收敛波动性，
即降低了参数更新的方差，使得更新更加稳定。相对于全量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。</p>
<p>一般情况下，小批量梯度下降是梯度下降的推荐变体，特别是在深度学习中。每次随机选择2的幂数个样本来进行学习，例如：8、16、32、64、128、256。因为计算机的结构就是二进制的。但是也要根据具体问题而选择，实践中可以进行多次试验，
选择一个更新速度与更次次数都较适合的样本数。</p>
<p>MBGD梯度下降迭代的收敛曲线更加温柔一些：</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/fa690fe760284383b198810802e82995.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure>
<h4 id="梯度下降优化">2.6、梯度下降优化</h4>
<p>虽然梯度下降算法效果很好，并且广泛使用，但是不管用上面三种哪一种，都存在一些挑战与问题，我们可以从以下几点进行优化:</p>
<ol type="1">
<li><p>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/0bbe5b04ddf24a78a71c4700cbde90a5.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
<li><p>学习速率调整，试图在每次更新过程中，
改变学习速率。从经验上看，<strong>学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。</strong>比较简单的学习率调整可以通过
<strong>学习率衰减（Learning Rate
Decay）</strong>的方式来实现。假设初始化学习率为 <span class="math inline">\(\eta_0\)</span>，在第 t 次迭代时的学习率 <span class="math inline">\(\eta_t\)</span>。常用的衰减方式为可以设置为
<strong>按迭代次数</strong> 进行衰减，迭代次数越大，学习率越小！ <img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/da9eec25972249f1a587a10d5723320f.jpeg" alt="image.png"></p>
<figure>
<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/def103855dc34212bd5cfda1686a73bb.jpeg" alt="image.png">
<figcaption aria-hidden="true">image.png</figcaption>
</figure></li>
<li><p>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的，或者每个特征有着不同的统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/f95707a6d51948c3a6aa7fcb61b5c055.jpg" alt="image.png"></p></li>
<li><p>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。<img src="https://fynotefile.oss-cn-zhangjiakou.aliyuncs.com/fynote/fyfile/463/1652339399091/ab1758d6f7504c128eee9bdb71045e4d.png" alt="image.png"></p>
<p>简单的问题，一般使用随机梯度下降即可解决。在深度学习里，对梯度下降进行了很多改进，比如：自适应梯度下降。在深度学习章节，我们会具体介绍。</p></li>
<li><p>轮次和批次</p>
<p>轮次：epoch，轮次顾名思义是把我们已有的训练集数据学习多少轮，迭代多少次。</p>
<p>批次：batch，批次这里指的的我们已有的训练集数据比较多的时候，一轮要学习太多数据，
那就把一轮次要学习的数据分成多个批次，一批一批数据的学习。</p>
<p>就好比，你要背诵一片《赤壁赋》，很长。你在背诵的时候，一段段的背诵，就是批次batch。花费了一天终于背诵下来了，以后的9天，每天都进行一轮背诵复习，这就是轮次epoch。这样，《赤壁赋》的背诵效果，就非常牢固了。</p>
<p>在进行，机器学习训练时，我们也要合理选择轮次和批次~</p></li>
</ol>
<h3 id="代码实战梯度下降">3、代码实战梯度下降</h3>
<h4 id="批量梯度下降bgd-1">3.1、批量梯度下降BGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率，逆时衰减</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">1000</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w)  + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X,np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次</span></span><br><span class="line">epoches = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、判断是否收敛，一般不会去设定阈值，而是直接采用设置相对大的迭代次数保证可以收敛</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">    <span class="comment"># 根据公式计算梯度</span></span><br><span class="line">    g = X.T.dot(X.dot(θ) - y)</span><br><span class="line">    <span class="comment"># 应用梯度下降的公式去调整 θ 值</span></span><br><span class="line">    learning_rate = learning_rate_schedule(i)</span><br><span class="line">    θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="随机梯度下降sgd-1">3.2、随机梯度下降SGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = <span class="number">2</span>*np.random.rand(<span class="number">100</span>, <span class="number">5</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、创建超参数轮次、样本数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">6</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n) <span class="comment"># 0 ~99</span></span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index] <span class="comment"># 打乱顺序</span></span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        X_i = X[[i]]</span><br><span class="line">        y_i = y[[i]]</span><br><span class="line">        g = X_i.T.dot(X_i.dot(θ)-y_i)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch*n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<h4 id="小批量梯度下降mbgd-1">3.3、小批量梯度下降MBGD</h4>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。一元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">w,b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">2</span>)</span><br><span class="line">y = w * X + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项x_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
<p><strong>这里我们使用了偏置项，即解决<span class="math inline">\(x_0^{(i)} =
1\)</span></strong>。多元一次线性回归问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、创建数据集X，y</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">3</span>)</span><br><span class="line">w = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = (<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">b = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,size = <span class="number">1</span>)</span><br><span class="line">y = X.dot(w) + b + np.random.randn(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、使用偏置项 X_0 = 1，更新X</span></span><br><span class="line">X = np.c_[X, np.ones((<span class="number">100</span>, <span class="number">1</span>))]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、定义一个函数来调整学习率</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">500</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_rate_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0/(t+t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、创建超参数轮次、样本数量、小批量数量</span></span><br><span class="line">epochs = <span class="number">10000</span></span><br><span class="line">n = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">num_batches = <span class="built_in">int</span>(n / batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、初始化 W0...Wn，标准正太分布创建W</span></span><br><span class="line">θ = np.random.randn(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6、多次for循环实现梯度下降，最终结果收敛</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 在双层for循环之间，每个轮次开始分批次迭代之前打乱数据索引顺序</span></span><br><span class="line">    index = np.arange(n)</span><br><span class="line">    np.random.shuffle(index)</span><br><span class="line">    X = X[index]</span><br><span class="line">    y = y[index]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batches):</span><br><span class="line">        <span class="comment"># 一次取一批数据16个样本</span></span><br><span class="line">        X_batch = X[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        y_batch = y[i * batch_size : (i + <span class="number">1</span>)*batch_size]</span><br><span class="line">        g = X_batch.T.dot(X_batch.dot(θ)-y_batch)</span><br><span class="line">        learning_rate = learning_rate_schedule(epoch * n + i)</span><br><span class="line">        θ = θ - learning_rate * g</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;真实斜率和截距是：&#x27;</span>,w,b)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;梯度下降计算斜率和截距是：&#x27;</span>,θ)</span><br></pre></td></tr></table></figure>
</div><div class="article-licensing box"><div class="licensing-title"><p>多元线性回归</p><p><a href="https://0914ds.github.io/2023/08/18/机器学习/梯度下降/">https://0914ds.github.io/2023/08/18/机器学习/梯度下降/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>albert dong</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-08-18</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-08-18</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="../../../../../" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="../../../../../" alt="Alipay"></span></a><a class="button donate" href="../../../../../" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="../../../../../" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="../../../../../" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="../../../17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="level-item">多元线性回归</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="../../../../../img/avatar.png" alt="albert"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">albert</p><p class="is-size-6 is-block">albert</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>shanghai</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="../../../../../archives"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="../../../../../categories"><p class="title">5</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="../../../../../tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/0914ds" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="../../../../../index.html"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#梯度下降"><span class="level-left"><span class="level-item">1</span><span class="level-item">梯度下降</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#线性回归预测房价"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">线性回归预测房价</span></span></a></li><li><a class="level is-mobile" href="#无约束最优化问题"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">1、无约束最优化问题</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#无约束最优化"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">1.1、无约束最优化</span></span></a></li><li><a class="level is-mobile" href="#梯度下降-1"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">1.2、梯度下降</span></span></a></li><li><a class="level is-mobile" href="#梯度下降公式"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">1.3、梯度下降公式</span></span></a></li><li><a class="level is-mobile" href="#学习率"><span class="level-left"><span class="level-item">1.2.4</span><span class="level-item">1.4、学习率</span></span></a></li><li><a class="level is-mobile" href="#全局最优化"><span class="level-left"><span class="level-item">1.2.5</span><span class="level-item">1.5、全局最优化</span></span></a></li><li><a class="level is-mobile" href="#梯度下降步骤"><span class="level-left"><span class="level-item">1.2.6</span><span class="level-item">1.6、梯度下降步骤</span></span></a></li><li><a class="level is-mobile" href="#代码模拟梯度下降"><span class="level-left"><span class="level-item">1.2.7</span><span class="level-item">1.7、代码模拟梯度下降</span></span></a></li></ul></li><li><a class="level is-mobile" href="#梯度下降方法"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">2、梯度下降方法</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#三种梯度下降不同"><span class="level-left"><span class="level-item">1.3.1</span><span class="level-item">2.1、三种梯度下降不同</span></span></a></li><li><a class="level is-mobile" href="#线性回归梯度更新公式"><span class="level-left"><span class="level-item">1.3.2</span><span class="level-item">2.2、线性回归梯度更新公式</span></span></a></li><li><a class="level is-mobile" href="#批量梯度下降bgd"><span class="level-left"><span class="level-item">1.3.3</span><span class="level-item">2.3、批量梯度下降BGD</span></span></a></li><li><a class="level is-mobile" href="#随机梯度下降sgd"><span class="level-left"><span class="level-item">1.3.4</span><span class="level-item">2.4、随机梯度下降SGD</span></span></a></li><li><a class="level is-mobile" href="#小批量梯度下降mbgd"><span class="level-left"><span class="level-item">1.3.5</span><span class="level-item">2.5、小批量梯度下降MBGD</span></span></a></li><li><a class="level is-mobile" href="#梯度下降优化"><span class="level-left"><span class="level-item">1.3.6</span><span class="level-item">2.6、梯度下降优化</span></span></a></li></ul></li><li><a class="level is-mobile" href="#代码实战梯度下降"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">3、代码实战梯度下降</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#批量梯度下降bgd-1"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">3.1、批量梯度下降BGD</span></span></a></li><li><a class="level is-mobile" href="#随机梯度下降sgd-1"><span class="level-left"><span class="level-item">1.4.2</span><span class="level-item">3.2、随机梯度下降SGD</span></span></a></li><li><a class="level is-mobile" href="#小批量梯度下降mbgd-1"><span class="level-left"><span class="level-item">1.4.3</span><span class="level-item">3.3、小批量梯度下降MBGD</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="../../../../../js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="../../../../../categories/JVM%E8%B0%83%E4%BC%98%E5%90%88%E9%9B%86/"><span class="level-start"><span class="level-item">JVM调优合集</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="../../../../../categories/%E5%AE%B9%E5%99%A8/"><span class="level-start"><span class="level-item">容器</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="../../../../../categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"><span class="level-start"><span class="level-item">数据库</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../../../../categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"><span class="level-start"><span class="level-item">消息中间件</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-17T17:13:16.000Z">2023-08-18</time></p><p class="title"><a href="">多元线性回归</a></p><p class="categories"><a href="../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-17T05:13:16.000Z">2023-08-17</time></p><p class="title"><a href="../../../17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">多元线性回归</a></p><p class="categories"><a href="../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-08-16T17:30:16.000Z">2023-08-17</time></p><p class="title"><a href="../../../17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96/">梯度下降优化</a></p><p class="categories"><a href="../../../../../categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-17T06:13:16.000Z">2023-06-17</time></p><p class="title"><a href="../../../../06/17/%E6%B6%88%E6%81%AF/kafka/">kafka</a></p><p class="categories"><a href="../../../../../categories/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/">消息中间件</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-06-05T07:02:29.235Z">2023-06-05</time></p><p class="title"><a href="../../../../06/05/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9A%84%E9%94%81%E6%9C%BA%E5%88%B6/">mysql的锁机制</a></p><p class="categories"><a href="../../../../../categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="../../../../../archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../../../../archives/2023/06/"><span class="level-start"><span class="level-item">June 2023</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="../../../../../archives/2023/05/"><span class="level-start"><span class="level-item">May 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="../../../../../archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><!--!--></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="../../../../../index.html"><img src="../../../../../img/logo.svg" alt="blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 albert dong</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="../../../../../js/column.js"></script><script src="../../../../../js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="../../../../../js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="../../../../../js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="../../../../../js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"../../../../../content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>